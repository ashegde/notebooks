{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yC091qfC5iT"
      },
      "source": [
        "In this notebook, we explore simple examples Recurrent Neural Networks -- on this case we look at Long Short-Term Memory (LSTM) networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vwg6RtuBwYa"
      },
      "source": [
        "As before, we keep in mind that neural networks which output logits (or possibly unnormalized probabilities) essentially parameterize a probability distribution over the data space. Recurrent Neural Networks (RNNs) provide a convenient way of parameterizing probability distributions over variable-length sequences, i.e., $\\mathbb{P}(x_1 x_2 ... x_T)$. Moreover, RNNs can be treated as generative models for these sequences.\n",
        "\n",
        "Key concepts to understand are (1) backpropagation and exploding/vanishing gradients, (2) notions of dynamics and memory, and (3) probability models, specifically the evolution from Bigram / Trigram / $N$-gram Markov Models to Hidden Markov Models to RNNs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS3T9u6MiQxp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!git clone https://github.com/karpathy/makemore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W17ZOd1TLez4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkGfCqJ1q_tD"
      },
      "outputs": [],
      "source": [
        "with open('makemore/names.txt','r') as file:\n",
        "  words = file.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKCjx5iWrAKY"
      },
      "outputs": [],
      "source": [
        "word_lengths = torch.tensor([len(w) for w in words]).float()\n",
        "print(\n",
        " f\"\"\"\n",
        " This dataset contains {word_lengths.nelement()} names\\n\n",
        " The minimum name length is {word_lengths.min()} characters.\\n \n",
        " The maximum name length is {word_lengths.max()} characters.\\n\n",
        " The mean name length is  {word_lengths.mean():.2f} characters. \\n\n",
        " The associated standard deviation is {word_lengths.std():.2f} characters.\n",
        " \"\"\"\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1NQVFWDrGQX"
      },
      "outputs": [],
      "source": [
        "#building the character vocabulary and lookup tables to map from characters to integer indices and back\n",
        "\n",
        "chars = ['>']+sorted(list(set(''.join(words))))+['<']  #'>' = start token, '<' = end token \n",
        "s_to_i = {s:i for i,s in enumerate(chars)}\n",
        "i_to_s = {i:s for s,i in s_to_i.items()}\n",
        "vocab_size = len(i_to_s)\n",
        "print(i_to_s)\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48XyvM0RjqFJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "word_lengths = torch.tensor([len(w) for w in words])\n",
        "max_word_length = word_lengths.max()\n",
        "\n",
        "def build_dataset(words):\n",
        "  '''\n",
        "  Build training, validation/dev, and test datasets.\n",
        "  Note, the training dataset is padded with special characters to ensure \n",
        "  each word/sequence is of the same length. This is only being done so we can train with minibatches.\n",
        "  Presumably, the model should learn that an end character '<' should be followed by another end character '<' with probability 1.\n",
        "  '''\n",
        "  # 80%, 10%, 10%\n",
        "  random.seed(42)\n",
        "  random.shuffle(words)\n",
        "  n1 = int(0.8*len(words))\n",
        "  n2 = int(0.9*len(words))\n",
        "\n",
        "  X = []\n",
        "  for w in words:\n",
        "    v = [s_to_i[ch] for ch in '>'+w+'<']\n",
        "    while len(v) < max_word_length+2: #padding, +2 for the start and end characters\n",
        "      v += [s_to_i['<']]\n",
        "    X.append(v)\n",
        "\n",
        "  Xtr = torch.tensor(X[:n1])\n",
        "  Xdev = torch.tensor(X[n1:n2])\n",
        "  Xte = torch.tensor(X[n2:])\n",
        "  \n",
        "  return Xtr, Xdev, Xte\n",
        "\n",
        "#training split (used to train parameters), dev/validation split (used to train hyperparameters), test split (at end with the final model)\n",
        "\n",
        "Xtr, Xdev, Xte = build_dataset(words)\n",
        "\n",
        "print(Xtr.size())\n",
        "print(Xdev.size())\n",
        "print(Xte.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7V_LsNJCD-m"
      },
      "source": [
        "## A digression on models\n",
        "As we have discussed, models in this setting are probability distributions over sequences $\\mathbb{P}(x_1 x_2 ... x_T)$. Some models are good and match the data, some models are bad and overfit or do not support the data. We often assume that the data are i.i.d. sampled sequences from an unknown true distribution $\\mathbb{P}_{\\textrm{true}}$. \n",
        "\n",
        "Markov models (e.g., n-gram models), hidden markov models, recurrent neural networks, etc. all correspond to different assumptions on the form of the model. Ideally, assumptions are supported by some underlying theory, but are often made out of necessity to render the problem computationally tractable. Without tractability, nothing can be accomplished. \n",
        "\n",
        "In the following, we investigate two simple \"extreme-case\" models to provide us with some benchmarks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn9SQigUkJyF"
      },
      "source": [
        "The first is a perfectly overfit model -- the empirical distribution of the training data. Consider the training dataset $\\mathcal{D}$ made up of iid samples $x^{(1)},...,x^{(N)} \\sim \\mathbb{P}_{\\textrm{true}}$. Then,\n",
        "\n",
        "$\\mathbb{P}_{\\textrm{emp}}(x|\\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^{N} 1_{x^{(i)}}(x)$,\n",
        "\n",
        "which assigns probability mass only to the observed realizations of the training data. Note that since the test data realizations are unlikely to match the training set, this model assigns them zero probability and thus utterly fails to generalize.\n",
        "\n",
        "With this probability model, the cross entropy between the empirical distribution (unbiased proxy for the unknown $\\mathbb{P}_{\\textrm{true}}$) and the model (which in this case is also the empirical distribution) is given by:\n",
        "\n",
        "CrossEntropyLoss1 \\\\\n",
        "$= -\\int \\mathbb{P}_{\\textrm{emp}}(x|\\mathcal{D}) \\log ( \\mathbb{P}_{\\textrm{emp}}(x|\\mathcal{D})) dx $ \\\\\n",
        "$= - \\frac{1}{N} \\sum_{i=1}^{N}  \\log ( \\mathbb{P}_{\\textrm{emp}}(x^{(i)} |\\mathcal{D})) $   &emsp;  note, this is the average negative log likelihood of the training data \\\\\n",
        "$= - \\frac{1}{N} \\sum_{i=1}^{N}  \\log ( \\frac{1}{N}) $ \\\\\n",
        "$= \\log(N)$\n",
        "\n",
        "Recall in our example above, $N=25626$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW1XT4-Wtcyg"
      },
      "outputs": [],
      "source": [
        "PerfectOverfittingCELoss = np.log(Xtr.shape[0])\n",
        "print(PerfectOverfittingCELoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqIMFjytdA1"
      },
      "source": [
        "In contrast, a model that does not overfit and completely underfits is the probability model that assigns all possible sequences equal probability. Suppose the sequence length is $T$ (as above) and that each token $x_i$ can take on $V$ different values, i.e., $V$ is the size of the vocabulary. Then, there are a total of $V^T$ possible sequences. In our example, $V=28$ and $T=17$. This model is perhaps the simplest model -- it has no dependence on the training data -- and is described by:\n",
        "\n",
        "$\\mathbb{P}_{\\textrm{unif}}(x) = \\frac{1}{V^T}$\n",
        "\n",
        "With this model, the cross-entropy loss is:\n",
        "\n",
        "CrossEntropyLoss2 \\\\\n",
        "$= -\\int \\mathbb{P}_{\\textrm{emp}}(x|\\mathcal{D}) \\log (\\mathbb{P}_{\\textrm{unif}}(x)) dx $ \\\\\n",
        "$= - \\frac{1}{N} \\sum_{i=1}^{N}  \\log ( \\mathbb{P}_{\\textrm{unif}}(x^{(i)} )) $  \\\\\n",
        "$= - \\frac{1}{N} \\sum_{i=1}^{N}  \\log ( \\frac{1}{V^T} ) $ \\\\\n",
        "$= \\log ( V^T)$ \\\\\n",
        "$= T \\log(V) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il2OMB_40vJY"
      },
      "outputs": [],
      "source": [
        "UniformCELoss = 17 * np.log(28)\n",
        "print(UniformCELoss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflBDzvcmXdv"
      },
      "source": [
        "The above models are quite simple, but they provide some benchmarks and help us interpret what values the loss should take. Losses near $10.15$ are likely to be overfitting, while losses near $56.65$ suggest the model does no better than a uniform prediction model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6WC5sUy1MW9"
      },
      "source": [
        "### A Simple LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pue0zq_DIRRO"
      },
      "source": [
        "First, we will build a single LSTM cell. As before we will do this in a pytorch-like fashion, but try not to use pre-built classes and methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrS3tA6zIXfM"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "# Training a 2-layer RNN resulting in a loss that grew. \n",
        "# Presumably, this was due to the gradients for certain weights vanishing.\n",
        "# Including layer normalization in the RecurrentBlock moderated this issue.\n",
        "\n",
        "class RecurrentDropout(nn.Module):\n",
        "  '''\n",
        "  Dropout method of Gal, Ghahramani - A Theoretically Grounded Application of Dropout in\n",
        "  Recurrent Neural Networks (2016).\n",
        "  '''\n",
        "  def __init__(self, p):\n",
        "    super(RecurrentDropout, self).__init__()\n",
        "    self.p = torch.tensor(p, requires_grad=False)\n",
        "    self.msk = None  \n",
        "\n",
        "  def set_mask(self, B, in_dim, device):\n",
        "    self.msk = torch.bernoulli(self.p*torch.ones((B,in_dim))).to(device) \n",
        "    self.msk.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "      #x is (B,in_dim)\n",
        "      return x * self.msk\n",
        "\n",
        "class LSTMBlock(nn.Module):\n",
        "  '''\n",
        "  Single LSTM layer\n",
        "  '''\n",
        "  def __init__(self, in_dim, hidden_dim, out_dim, p):\n",
        "    super(LSTMBlock, self).__init__()\n",
        "    #\n",
        "    self.in_dim = in_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.out_dim = out_dim\n",
        "    #internal states\n",
        "    self.h = None #hidden state\n",
        "    self.c = None #cell memory\n",
        "    self.hidden_dim = hidden_dim\n",
        "    # dropout layers\n",
        "    self.dropout_i = RecurrentDropout(1-p)\n",
        "    self.dropout_h = RecurrentDropout(1-p)\n",
        "    # state transition\n",
        "    self.layer_hh = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "    self.layer_ih = nn.Linear(in_dim, hidden_dim, bias=False)\n",
        "    self.tanh = torch.tanh \n",
        "    #gates\n",
        "    self.sigmoid = torch.sigmoid\n",
        "    # input gate\n",
        "    self.layer_hh_ig = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "    self.layer_ih_ig = nn.Linear(in_dim, hidden_dim, bias=False)\n",
        "    # forget gate\n",
        "    self.layer_hh_fg = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "    self.layer_ih_fg = nn.Linear(in_dim, hidden_dim, bias=False)\n",
        "    # output gate\n",
        "    self.layer_hh_og = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "    self.layer_ih_og = nn.Linear(in_dim, hidden_dim, bias=False)\n",
        "    #cell output\n",
        "    self.layer_ho = nn.Linear(hidden_dim, out_dim, bias=True)\n",
        "    #mode\n",
        "    self.train=True\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x is (B,in_dim), h is (B, hidden_dim)\n",
        "\n",
        "    if self.train:\n",
        "      self.h = self.dropout_h(self.h)\n",
        "      x = self.dropout_i(x)\n",
        "\n",
        "    ig = self.sigmoid( self.layer_hh_ig(self.h) + self.layer_ih_ig(x) )\n",
        "    fg = self.sigmoid( self.layer_hh_fg(self.h) + self.layer_ih_fg(x) )\n",
        "    og = self.sigmoid( self.layer_hh_og(self.h) + self.layer_ih_og(x) )\n",
        "    hprop = self.tanh( self.layer_hh(self.h) + self.layer_ih(x) )\n",
        "    self.c = fg * self.c + ig * hprop\n",
        "    self.h = og * self.tanh(self.c)\n",
        "    y = self.layer_ho(self.h) # (B,out_dim)\n",
        "    return y\n",
        "\n",
        "  def set_h(self, hnew, dev):\n",
        "    self.h = hnew.to(dev)\n",
        "\n",
        "  def set_c(self, cnew, dev):\n",
        "    self.c = cnew.to(dev)\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "  '''\n",
        "  LSTM that accepts input tensors of dimension (T,B,I) where\n",
        "  T = maximum length of the sequence (x1, x2, x3,...,xT)\n",
        "  B = batch dimension\n",
        "  I = input dimension\n",
        "  So if the input tensor is x, then x[0,1,:] = the embedding of the first token in the second example sequence of the batch \n",
        "  '''\n",
        "  def __init__(self, vocab_size, n_embd, block_dims, p, device):\n",
        "    super(LSTM, self).__init__()\n",
        "    # block_dims = [(in_dim, hidden_dim, out_dim) for all layers]\n",
        "    self.depth = len(block_dims)\n",
        "    self.device = device\n",
        "    assert( all(block_dims[ii][2] == block_dims[ii+1][0] for ii in range(self.depth-1)) ) #check compatibility\n",
        "    self.embedding = nn.Embedding(vocab_size, n_embd)\n",
        "    self.layers = nn.Sequential(*[LSTMBlock(*d, p) for d in block_dims])\n",
        "\n",
        "  def step(self, x):\n",
        "    # x is (B,in_dim)\n",
        "    y = self.layers[0](x)\n",
        "    if self.depth > 1:\n",
        "      for layer in self.layers[1:]:\n",
        "        y = layer(y)\n",
        "    return y\n",
        "\n",
        "  def forward(self, z):\n",
        "    emb = self.embedding(z) #(batch_size, max_word_length, n_embd), embeds characters into vector space\n",
        "    x = emb.permute(1,0,2)\n",
        "\n",
        "    T,B,I = x.size()\n",
        "\n",
        "    for layer in self.layers: #init states, dropout\n",
        "      layer.set_h(torch.zeros(layer.hidden_dim,), self.device)\n",
        "      layer.set_c(torch.zeros(layer.hidden_dim,), self.device)\n",
        "      if layer.train:\n",
        "        layer.dropout_i.set_mask(B,layer.in_dim, self.device)\n",
        "        layer.dropout_h.set_mask(B,layer.hidden_dim, self.device)\n",
        "\n",
        "    logits = torch.zeros((T-1,B,vocab_size)) \n",
        "    for t in range(T-1):  \n",
        "      logits[t]= self.step(x[t])\n",
        "    return logits\n",
        "\n",
        "  def eval(self):\n",
        "    for layer in self.layers:\n",
        "      layer.train = False\n",
        "\n",
        "  def train(self):\n",
        "    for layer in self.layers:\n",
        "      layer.train = True\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPqTydnQk-Lg"
      },
      "outputs": [],
      "source": [
        "n_embd = 20\n",
        "n_hidden = 512\n",
        "p = 0.5 #dropout rate\n",
        "model = LSTM( vocab_size, n_embd, [(n_embd, n_hidden, vocab_size)],p, device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-VO7SlBfROD"
      },
      "outputs": [],
      "source": [
        "print(sum(p.nelement() for p in model.parameters() ))\n",
        "for p in model.parameters():\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAL-hd60IpiN"
      },
      "outputs": [],
      "source": [
        "max_steps = 200000\n",
        "batch_size = 128\n",
        "Xdev = Xdev.to(device)\n",
        "\n",
        "lossi = []\n",
        "model.train\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # constructing minibatch \n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb = Xtr[ix].to(device) #(batch_size, max_word_length)\n",
        "\n",
        "  logits = model(Xb).to(device) #logits is (max_word_length, batch_size, vocab_size)\n",
        "\n",
        "  loss = 0\n",
        "  for t in range(max_word_length-1):\n",
        "    loss += F.cross_entropy(logits[t],Xb[:,t+1]) #cross entropy loss averaged over time steps (and minibatch)\n",
        "\n",
        "  # backward pass\n",
        "  for p in model.parameters():\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i<100000 else 0.01\n",
        "  for p in model.parameters():\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  if i % 10000 == 0:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits_dev = model(Xdev).to(device) #logits is (max_word_length, batch_size, vocab_size)\n",
        "      loss_dev = 0\n",
        "      for t in range(max_word_length-1):\n",
        "        loss_dev += F.cross_entropy(logits_dev[t],Xdev[:,t+1])\n",
        "    model.train()\n",
        "    print(f'{i:7d} / {max_steps:7d}  |  batch loss = {loss.item():.4f}  | dev loss = {loss_dev.item():.4f} ') #prints the batch loss\n",
        "\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVJh-F363W0h"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1saR8d6vJCO1"
      },
      "outputs": [],
      "source": [
        "# training loss\n",
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  x = { \n",
        "      'train': (Xtr),\n",
        "      'val': (Xdev),\n",
        "      'test': (Xte),\n",
        "  }[split]\n",
        "  x=x.to(device)\n",
        "\n",
        "  logits = model(x).to(device) #logits is (max_word_length, batch_size, vocab_size)\n",
        "  loss = 0\n",
        "  for t in range(max_word_length-1):\n",
        "    loss += F.cross_entropy(logits[t],x[:,t+1])\n",
        "  print(split, loss.item())\n",
        "  \n",
        "split_loss('train')\n",
        "split_loss('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKvylD7gJFGg"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647 + 11)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(20):\n",
        "    out = []\n",
        "    current_input = torch.tensor([s_to_i['>']]).to(device)  \n",
        "    for layer in model.layers:\n",
        "      layer.set_h(torch.zeros(layer.hidden_dim,), device)\n",
        "      layer.set_c(torch.zeros(layer.hidden_dim,), device)\n",
        "    while True:\n",
        "      x=model.embedding(current_input) #(1,n_embd)\n",
        "      #z = emb.permute(1,0,2)\n",
        "      logits = model.step(x).to(device) #logits = ()\n",
        "      probs = F.softmax(logits,dim=1) # get the output distribution on the characters\n",
        "      ix = torch.multinomial(probs.squeeze(), num_samples=1).item()\n",
        "      current_input =  torch.tensor([ix]).to(device)   #shifts the context window to the right one character (now includes ix)\n",
        "      out.append(ix)\n",
        "      if ix == s_to_i['<']: #terminate at the end character '<'\n",
        "        break\n",
        "    print(''.join(i_to_s[i] for i in out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvx-d3HwG2xU"
      },
      "outputs": [],
      "source": [
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-M-dPDmmU1"
      },
      "source": [
        "##Deep LSTM\n",
        "\n",
        "Next we will try connecting two LSTM blocks to improve the results. Recall that deeper architectures can be harder to train, particularly using just vanilla stochastic gradient descent and no other bells and whistles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L_elhtRmrUy"
      },
      "outputs": [],
      "source": [
        "n_embd = 20\n",
        "n_hidden = 512\n",
        "p = 0.5 #dropout rate\n",
        "\n",
        "model = LSTM(vocab_size, n_embd, [(n_embd, n_hidden, n_hidden), (n_hidden, n_hidden, vocab_size)],p,  device) #2 RNN blocks\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr3QexprCE5E"
      },
      "outputs": [],
      "source": [
        "print(sum(p.nelement() for p in model.parameters() ))\n",
        "for p in model.parameters():\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLuLDWvHmzK_"
      },
      "outputs": [],
      "source": [
        "max_steps = 200000\n",
        "batch_size = 128\n",
        "Xdev = Xdev.to(device)\n",
        "\n",
        "lossi = []\n",
        "model.train\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # constructing minibatch \n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb = Xtr[ix].to(device) #(batch_size, max_word_length)\n",
        "\n",
        "  logits = model(Xb).to(device) #logits is (max_word_length, batch_size, vocab_size)\n",
        "\n",
        "  loss = 0\n",
        "  for t in range(max_word_length-1):\n",
        "    loss += F.cross_entropy(logits[t],Xb[:,t+1]) #cross entropy loss averaged over time steps (and minibatch)\n",
        "\n",
        "  # backward pass\n",
        "  for p in model.parameters():\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i<100000 else 0.01\n",
        "  for p in model.parameters():\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  if i % 10000 == 0:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits_dev = model(Xdev).to(device) #logits is (max_word_length, batch_size, vocab_size)\n",
        "      loss_dev = 0\n",
        "      for t in range(max_word_length-1):\n",
        "        loss_dev += F.cross_entropy(logits_dev[t],Xdev[:,t+1])\n",
        "    model.train()\n",
        "    print(f'{i:7d} / {max_steps:7d}  |  batch loss = {loss.item():.4f}  | dev loss = {loss_dev.item():.4f} ') #prints the batch loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyahX7El7EMt"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr14SM31m4Oa"
      },
      "outputs": [],
      "source": [
        "# training loss\n",
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  x = { \n",
        "      'train': (Xtr),\n",
        "      'val': (Xdev),\n",
        "      'test': (Xte),\n",
        "  }[split]\n",
        "  x=x.to(device)\n",
        "\n",
        "  logits = model(x).to(device) #logits is (max_word_length, batch_size, vocab_size)\n",
        "  loss = 0\n",
        "  for t in range(max_word_length-1):\n",
        "    loss += F.cross_entropy(logits[t],x[:,t+1])\n",
        "  print(split, loss.item())\n",
        "  \n",
        "split_loss('train')\n",
        "split_loss('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Dl-Op0m_Ce"
      },
      "outputs": [],
      "source": [
        "g = torch.Generator().manual_seed(2147483647 + 11)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(20):\n",
        "    out = []\n",
        "    current_input = torch.tensor([s_to_i['>']]).to(device)  \n",
        "    for layer in model.layers:\n",
        "      layer.set_h(torch.zeros(layer.hidden_dim,), device)\n",
        "      layer.set_c(torch.zeros(layer.hidden_dim,), device)\n",
        "    while True:\n",
        "      x=model.embedding(current_input) #(1,n_embd)\n",
        "      #z = emb.permute(1,0,2)\n",
        "      logits = model.step(x).to(device) #logits = ()\n",
        "      probs = F.softmax(logits,dim=1) # get the output distribution on the characters\n",
        "      ix = torch.multinomial(probs.squeeze(), num_samples=1).item()\n",
        "      current_input =  torch.tensor([ix]).to(device)   #shifts the context window to the right one character (now includes ix)\n",
        "      out.append(ix)\n",
        "      if ix == s_to_i['<']: #terminate at the end character '<'\n",
        "        break\n",
        "    print(''.join(i_to_s[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}