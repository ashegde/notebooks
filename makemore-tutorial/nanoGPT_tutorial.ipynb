{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yC091qfC5iT"
      },
      "source": [
        "The material in this notebook follows Andrej Karpathy's nanoGPT tutorial (https://www.youtube.com/watch?v=kCc8FmEb1nY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hS3T9u6MiQxp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the TinyShakespeare Dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZYCx9z5Pioj",
        "outputId": "867b3361-fae3-4cb3-d4ba-02906013906f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-22 05:41:00--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-01-22 05:41:00 (22.1 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "osWkIVz2P3IK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The dataset contains {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNIqZzXAP-Pc",
        "outputId": "9b95a09c-489d-4e34-deb1-fc0cd9c9e754"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset contains 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text)) #this dataset is just a big string (not a list of strings, we haven't used split or anything)\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8IQA1snTfVd",
        "outputId": "13442c19-ecb3-4adc-bea5-5a043518c101"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the unique characters/symbols/atoms that build the dataset\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7LvCMHkTvCb",
        "outputId": "6b165b08-a29c-42b3-e7c9-691e5b66e279"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOKENIZER\n",
        "# map characters to integers and vice versa\n",
        "# this corresponds to a character level tokenizer\n",
        "# state of the art methods tend to use subword level tokenizers\n",
        "#\n",
        "# note that although simple, character level tokenizers tend to produce very long sequences compared to other tokenizers.\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # converts string (list of characters) to integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) #converts list of integers to a single string (list of characters)\n",
        "\n",
        "print(encode('hello there'))\n",
        "print(decode(encode('hello there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiwneHFcUZBm",
        "outputId": "e7f46378-a2c6-49f7-9959-58a17caf2a79"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
            "hello there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the dataset and storing as a tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "plt.hist(data, vocab_size)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "O-SYmufNYJMN",
        "outputId": "c53e9320-c987-4a4d-b3d1-577db3318148"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The most occuring character is: \"{itos[1]}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DINvv5fZZRTV",
        "outputId": "046ea7b5-dd9d-4fdd-fb82-796c9913a5f0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most occuring character is: \" \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Training and validation split of the data\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "sFMe62mdZoA8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most applications, the training dataset is massive to be fed into the transformer all at once. Instead, training is performed by randomly sampling blocks or subsequences from the training set of a specified size, which is refered to here as a $\\texttt{block_size}$ or a $\\texttt{context_length}$. These blocks are ordered and contain multiple pieces of information. For example, consider  $\\texttt{block_size} = N$, e.g., \n",
        "\n",
        "$[1,...,N]$\n",
        "\n",
        "As we are interested in predicting next tokens, we can create the following list of training examples from that single block with the format $\\texttt{target}$ | $\\texttt{input}$:\n",
        "\n",
        "$1|$\n",
        "\n",
        "$2|1$\n",
        "\n",
        "$3|2,1$\n",
        "\n",
        "...\n",
        "\n",
        "$N | N-1,....,1$\n",
        "\n",
        "Hence the training using contexts of size $1$ all the way up to $\\texttt{block_size}$ "
      ],
      "metadata": {
        "id": "UJIZwk2ZbHqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here is an example of the above\n",
        "\n",
        "block_size = 8 # context length\n",
        "print(f'The first training block is {train_data[:block_size+1]}')\n",
        "print(f'From this block, we can construct the following prediction cases.')\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f'When the input is {context}, the target is {target}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umHgmoC1aP8K",
        "outputId": "7e4e64d6-971a-442b-ba17-fddacc8806e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first training block is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
            "From this block, we can construct the following prediction cases.\n",
            "When the input is tensor([18]), the target is 47.\n",
            "When the input is tensor([18, 47]), the target is 56.\n",
            "When the input is tensor([18, 47, 56]), the target is 57.\n",
            "When the input is tensor([18, 47, 56, 57]), the target is 58.\n",
            "When the input is tensor([18, 47, 56, 57, 58]), the target is 1.\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1]), the target is 15.\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47.\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall the analogy with joint probabilities. For a sequence $\\{t_1,...,t_N\\}$, the joint probability has the following factorization:\n",
        "\n",
        "$p(t_1,...,t_N) = p(t_1) p(t_2|t_1) p(t_3| t_2,t_1) ... p(t_{N} | t_{N-1},...,t_1)$ "
      ],
      "metadata": {
        "id": "5I7_iojTig0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers are not trained on individual blocks, but rather minibatches of multiple blocks. Hence, the $\\texttt{batch_size}$ is also an important hyperparameter. The primary reason for doing this is for efficiency.  "
      ],
      "metadata": {
        "id": "0llrlM9qj4LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Some default hyperparameters, which we will change throughout\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32"
      ],
      "metadata": {
        "id": "-w3Npa5ECkdr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent blocks/sequences will we process in parallel?\n",
        "block_size = 8 # what is the max context length for predictions?\n",
        "\n",
        "def get_batch(opt):\n",
        "  data = train_data if opt == 'train' else val_data\n",
        "  #select n random starting indices for a sequence of size block_size, where n = batch_size\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]) #create each block at each starting location in ix\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #create targets for each block in the batch\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x, y \n",
        "\n",
        "#next, we sample a batch from the training data set\n",
        "xb, yb = get_batch('train') \n",
        "\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "#below we unpack all of the examples stored in each block in the batch\n",
        "for b in range(batch_size): #b = batch\n",
        "  for t in range(block_size): #t = time\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f'When input is {context.tolist()}, the target is: {target}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilkwHvvmi8GX",
        "outputId": "82af699c-b845-4aa7-fcd1-d2721ad41afe"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "When input is [24], the target is: 43\n",
            "When input is [24, 43], the target is: 58\n",
            "When input is [24, 43, 58], the target is: 5\n",
            "When input is [24, 43, 58, 5], the target is: 57\n",
            "When input is [24, 43, 58, 5, 57], the target is: 1\n",
            "When input is [24, 43, 58, 5, 57, 1], the target is: 46\n",
            "When input is [24, 43, 58, 5, 57, 1, 46], the target is: 43\n",
            "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\n",
            "When input is [44], the target is: 53\n",
            "When input is [44, 53], the target is: 56\n",
            "When input is [44, 53, 56], the target is: 1\n",
            "When input is [44, 53, 56, 1], the target is: 58\n",
            "When input is [44, 53, 56, 1, 58], the target is: 46\n",
            "When input is [44, 53, 56, 1, 58, 46], the target is: 39\n",
            "When input is [44, 53, 56, 1, 58, 46, 39], the target is: 58\n",
            "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\n",
            "When input is [52], the target is: 58\n",
            "When input is [52, 58], the target is: 1\n",
            "When input is [52, 58, 1], the target is: 58\n",
            "When input is [52, 58, 1, 58], the target is: 46\n",
            "When input is [52, 58, 1, 58, 46], the target is: 39\n",
            "When input is [52, 58, 1, 58, 46, 39], the target is: 58\n",
            "When input is [52, 58, 1, 58, 46, 39, 58], the target is: 1\n",
            "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\n",
            "When input is [25], the target is: 17\n",
            "When input is [25, 17], the target is: 27\n",
            "When input is [25, 17, 27], the target is: 10\n",
            "When input is [25, 17, 27, 10], the target is: 0\n",
            "When input is [25, 17, 27, 10, 0], the target is: 21\n",
            "When input is [25, 17, 27, 10, 0, 21], the target is: 1\n",
            "When input is [25, 17, 27, 10, 0, 21, 1], the target is: 54\n",
            "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "m_sOeG1yCwK_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qek_5t9BCwRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to build up to a Transformer network, we will first recall the Bigram language model."
      ],
      "metadata": {
        "id": "YM96tpnNyTCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table.\n",
        "    # This lookup table is basically a transition probability matrix, where the probabilities are replaced by logits.\n",
        "    # Hence, entry (i,j) of the table corresponds to the logit associated with transitioning from the current token i the next token j.\n",
        "    # Thus the i-th row contains the logits (probability distribution over the vocabulary) for the next token.\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #(C,C)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    logits = self.token_embedding_table(idx) #(B,T,C)\n",
        "    # logits(b,t,:) contains the logits for predicting the next token given that we are currently at token t in batch b.\n",
        "    # these logits correspond to a probability distribution over the vocabulary.\n",
        "    \n",
        "    # we need to reshape things to work with F.cross_entropy\n",
        "    \n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "    # max_new_tokens is how many new tokens to sample\n",
        "\n",
        "    ## Note: In this simple bigram model, we only need to keep track of the latest indices and not the entire history.\n",
        "    ##       In the following code, we actually keep track of all of the indices in 'idx' and continually append to it.\n",
        "    ##       This is because we will need this capability in subsequent models that have more context.\n",
        "    for _ in range(max_new_tokens):\n",
        "      # make the predictions (probability distributions over next entries)\n",
        "      logits, loss = self(idx) # logits is (B,T,C), loss is (B*T)\n",
        "      # focus on the last time step because that is the only context needed for bigram predictions\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      # convert logits to probs\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      # sample\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      # appending new sampled indices to our current set\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "63qpX3B-yasf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-vHMOtAYW8S",
        "outputId": "201c2fa1-9afa-4b36-dc18-af8f4dc541c0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.0364, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = m.generate(xb, 20)\n",
        "\n",
        "decode(preds[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s06pI6HHcdWl",
        "outputId": "ff5a1d82-4a0c-4d09-8df8-1a9ca3ef9897"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's heXJamq!iui$HDXDFZuBSu\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2TOvxb4e-49",
        "outputId": "0916f8b4-03f2-4c63-a09b-e0464950deff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "klcPU cL'DpdLCafBheHd:.?nA!LgjgL&jcM-sZW!HFsWt'rv.x;z&hPyd'XHOxnAcj.erRjcLxxVFSrOpW.oeGJ Ai$yi&!mRfeopx,f.e..iBgxVrl-VThV?W?qF;roi!NxbR;zyDlfd&DH'X;PgqTOMcL\n",
            "HvR;tg;tEkFST$Cil;AkkEptkGClcJ NoPTeZLPX.B-aft'ZEl'rJfEDm&UZ\n",
            "RENz-sr rR:Z'IU,V-PWgQlYc-Gq$$yg;TvDY?ACacM.NkPHGpGJakq!3M!HOhbvD.llo;r;lpXezY;.LOMm!q$AYf$XyG$xWSufzLpMafJsqGbbRRMqNZtSbbKIIUCqVHyVeD\n",
            "PWyYswQKBHvb.CoPUCQA& :OTT fzrauAYI!3SU hP xWt'vkAHpniwCy,XyHWvXv.YJsixViwxaxysgOy,\n",
            "ipQwl ItwYD\n",
            ";erIZxaX,HAzloSmjtI?YMieH3Z jaNs\n",
            "aVCyddL\n",
            "N,OpZty;aK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously, this purely random model spits out gibberish. Now we will train this model on the tinyShakespeare dataset."
      ],
      "metadata": {
        "id": "L5_wsQr4gB6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32"
      ],
      "metadata": {
        "id": "SfLLzcKNz_4F"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "T9ErtX4pgJCL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for steps in range(10000):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1dYiJUvmwNs",
        "outputId": "4c94d8f4-2487-4301-f7fb-69096b705321"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5369927883148193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWS-PFWPoNTm",
        "outputId": "70a2ab86-7bf8-4ef3-8406-be11dce9bad0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3bllllleyomerer but?\n",
            "The t,\n",
            "Ith'dwitile w? beren to'd ff a atrts brey s\n",
            "\n",
            "ESesenther:\n",
            "Ithon f at pare ismamy an flictong m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Mameld h che IN: an y is aslo'daDut, t thethiceve fur t anowik\n",
            "Wirghe f bot d at'prd\n",
            "Anoper sof usy be, d s me cks bity.\n",
            "CLUM3&HALou f lendys.\n",
            "Y anditont avenghe m, gs gl tis y.\n",
            "Wie gh-mmo hizy s on fulourachigethuiclotif qDWeZPld:\n",
            "LOubour Witamul we thiech l lisowarrew bland cedanidate, fafive witherthiulsosthis thatwancaurind th'gonimake\n",
            "\n",
            "S oveprene?\n",
            "HRar oumnanoupamak \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the model is still incomprehensible, it is considerably improved over the untrained model. At least some of the structure of has been learned. Bigram models are quite poor, but they at least provide a benchmark for comparison. We need longer contexts than just 1."
      ],
      "metadata": {
        "id": "QZDh5UBdoUN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient implementations of self-attention use the following trick\n",
        "\n",
        "Basically, to ensure causal interactions via masking. By causal, we mean that the future cannot communicate with the past. This is a form of autoregression and highlights the role of lower triangular matrices."
      ],
      "metadata": {
        "id": "A4DEN456qCCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 #batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_18S2DU6otof",
        "outputId": "a0cde751-fa9f-4c65-b0df-059d5c4b6255"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want a causal temporal average:\n",
        "#  x[b,t] = mean_{i<=t} x[b,i]\n",
        "# bow = bag of words, averaging over previous words\n",
        "# all notion of order is loss, so some information is loss\n",
        "xbow = torch.zeros((B,T,C)) \n",
        "for b in range(B): #for each sequence in the batch\n",
        "  for t in range(T): #for each time\n",
        "    xprev = x[b,:t+1] #(t,C), includes the t-th entry\n",
        "    xbow[b,t] = torch.mean(xprev,0) #(C) average across the previous times"
      ],
      "metadata": {
        "id": "-aOayWc_rO5U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A more efficient formulation than for loops\n",
        "# causal averaging operator\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / a.sum(axis=1, keepdim=True)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KSu2yFttQIp",
        "outputId": "65a6e7fc-7e48-4d63-af84-f5d2b4150c12"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T,T)) #temporal averaging weights\n",
        "wei = wei / wei.sum(axis=1, keepdim=True)\n",
        "\n",
        "xbow2 = wei @ x #(T,T) @ (B,T,C) ----> (B,T,C) due to broadcasting, batch calculations done in parallel "
      ],
      "metadata": {
        "id": "UUG9pTpysbqN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( torch.allclose(xbow,xbow2) )\n",
        "print(xbow[0], xbow2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9eRDNlHuDKC",
        "outputId": "9b072dcf-9444-42d2-ec4d-0a31e30008bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]]) tensor([[ 0.1808, -0.0700],\n",
            "        [-0.0894, -0.4926],\n",
            "        [ 0.1490, -0.3199],\n",
            "        [ 0.3504, -0.2238],\n",
            "        [ 0.3525,  0.0545],\n",
            "        [ 0.0688, -0.0396],\n",
            "        [ 0.0927, -0.0682],\n",
            "        [-0.0341,  0.1332]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can write the same averaging operator with a softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n",
        "\n",
        "# The averaging operator 'wei' is currently a uniform average.\n",
        "# In reality, we may want a more general weighted average (but still causal), where the weights can be learned."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxKB3rrDwKxM",
        "outputId": "2fd7c7a5-9dea-4085-cba0-8db6ce52fb57"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expanding the Bigram Model\n",
        "We will begin to expand the previous bigram model with seemingly redundant information. These \"redundant\" aspects, such as token and position embeddings become important when dealing with Transformer models."
      ],
      "metadata": {
        "id": "sbnBxf7EzyPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32"
      ],
      "metadata": {
        "id": "CTTsxR1v0iae"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table.\n",
        "    # This lookup table is basically a transition probability matrix, where the probabilities are replaced by logits.\n",
        "    # Hence, entry (i,j) of the table corresponds to the logit associated with transitioning from the current token i the next token j.\n",
        "    # Thus the i-th row contains the logits (probability distribution over the vocabulary) for the next token.\n",
        "\n",
        "    # first we embed each input token into euclidean space (rather than using a one-hot vector)\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #(vocab_size,n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) #(block_size, n_embd)\n",
        "    # Note the token_embedding only embeds information about the value of the token, but not its position in the context\n",
        "    # Hence, we also include a position_embedding in the same space, basically as an additional learnable degree of freedom to account for the positioning within the context.\n",
        "    # Importantly, the position_embedding is only used for position information and not for conveying token value. Hence it is the same within the batch.\n",
        "\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    # logits(b,t,:) contains the logits for predicting the next token given that we are currently at token t in batch b.\n",
        "    # these logits correspond to a probability distribution over the vocabulary.\n",
        "    \n",
        "    # we need to reshape things to work with F.cross_entropy\n",
        "    \n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "    # max_new_tokens is how many new tokens to sample\n",
        "\n",
        "    ## Note: In this simple bigram model, we only need to keep track of the latest indices and not the entire history.\n",
        "    ##       In the following code, we actually keep track of all of the indices in 'idx' and continually append to it.\n",
        "    ##       This is because we will need this capability in subsequent models that have more context.\n",
        "    for _ in range(max_new_tokens):\n",
        "      # make the predictions (probability distributions over next entries)\n",
        "      logits, loss = self(idx) # logits is (B,T,C), loss is (B*T)\n",
        "      # focus on the last time step because that is the only context needed for bigram predictions\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      # convert logits to probs\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      # sample\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      # appending new sampled indices to our current set\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "logits, loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "nMI1fnAbz0yZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The crux of attention"
      ],
      "metadata": {
        "id": "dlO3Wiga4783"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#recap\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 #batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "# x[i,:,:] is a (T,C) snapshot matrix, where the t-th row x[i,t,:] contains the C-dimensional embedding of the t-th token\n",
        "# so the rows of x[i,:,:] index the time steps\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x # causal time averages of each dimension of the embedding"
      ],
      "metadata": {
        "id": "jsj1uHr94-Co"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With attention, we want to achieve something similar but more flexible. We still want to have information propogate in a causal way -- meaning that future tokens cannot influence the past. In constrast with the previous time averaging, however, we want this to be achieved in a data-dependent way.\n",
        "\n",
        "To do this, we abstract the notion of weights and averaging to the notion of affinities and similarity. Such concepts have been fruitfull applied with respect to manifold learning and kernel methods on graphs. The transformer perspective is as follows.\n",
        "\n",
        "Each token emits a query vector and a key vector in the same space. Key vectors contain information pertaining to the token's identity and query vectors contain information about what the token is looking for. We use \"information\" here in the loose sense as these key and query vectors constructed by a linear transformation of the input, which is learned from data. Each token communicates with all other tokens by taking the dot product between its own query vector and all other key vectors. These dot products are \"affinities\".\n",
        "\n",
        "The key and query vectors are created by linear transformation of the input vector. Why not use an emmbedding table? Perhapsto reduce the number of parameters.\n",
        "\n",
        "Attention is a communication mechanism between tokens in a sequence. We can interpret this as a directed graph."
      ],
      "metadata": {
        "id": "Wum6S0Ig8gqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A single attention head\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 #batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False) # (C, head_size)\n",
        "query = nn.Linear(C, head_size, bias=False) # (C, head_size)\n",
        "value = nn.Linear(C, head_size, bias=False) #(C, head_size)\n",
        "## x[i,:,:] is a (T,C) matrix of time snapshots in the rows\n",
        "## x[i,:,:] @ key is a (T,head_size)\n",
        "k = key(x) #(B,T,head_size)\n",
        "q = query(x) #(B,T,head_size)\n",
        "v = value(x) #(B,T,head_size)\n",
        "wei = q @ k.swapaxes(-2,-1) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
        "wei = wei * (head_size)**(-0.5) #scaling / normalization\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf')) #ensures causal masking\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ v #(B,T, head_size)"
      ],
      "metadata": {
        "id": "DdNQvT46_ggt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7PG5JaLNPAb",
        "outputId": "939ab63c-71c5-4c91-9a24-cc93cc73219d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Formalizing\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" One-headed self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # torch will recognize that 'tril' is not a model parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape #batch, time, channel\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.query(x) #(B,T,C)\n",
        "    v = self.value(x) #(B,T,C)\n",
        "\n",
        "    wei = q @ k.swapaxes(-2,-1) * C**(-0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "\n",
        "    wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
        "    out = wei @ v #(B,T,C)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "dq6TvY3PQufo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = SelfAttentionModel()\n",
        "logits, loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "SbYJKLxFTJtX"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF8rfS9P_d0A",
        "outputId": "b55c5721-ee3f-4093-d8b6-4eb9959ef1f8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 | train: 2.354154586791992 | test: 2.3653905391693115\n",
            "iter 1000 | train: 2.340414524078369 | test: 2.366382360458374\n",
            "iter 2000 | train: 2.3607404232025146 | test: 2.361701726913452\n",
            "iter 3000 | train: 2.333624839782715 | test: 2.374727249145508\n",
            "iter 4000 | train: 2.347350597381592 | test: 2.368013381958008\n",
            "iter 5000 | train: 2.345977544784546 | test: 2.3695223331451416\n",
            "iter 6000 | train: 2.3385987281799316 | test: 2.36625337600708\n",
            "iter 7000 | train: 2.3538753986358643 | test: 2.3672924041748047\n",
            "iter 8000 | train: 2.3466596603393555 | test: 2.3608691692352295\n",
            "iter 9000 | train: 2.333599805831909 | test: 2.3701629638671875\n",
            "2.4730064868927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lL5QP3QAUvC",
        "outputId": "6bddddc3-ef3b-47c0-fd20-4fdeb0ace6e4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bercl-\n",
            "Jupr ane mud pereve, wherwabae wang throm, het aw beghe tyorue-or I:\n",
            "Bt fomyar' feldid our Id itfr ss\n",
            "Wh\n",
            "Leayis hat thare was mbe, hangl ghe forevere ildescod ng;\n",
            "\n",
            "mark sct thealy fit tho het,\n",
            "Nom,\n",
            "And langea, te, mey phat blur wen ca aloy averi,\n",
            "hee.\n",
            "\n",
            "Thee.\n",
            "\n",
            "In:\n",
            "Cthe toed es:\n",
            "Se pore to ay owasp my,\n",
            "Moecou ge belullitut tham wion, I neg faven whasly ld tloorp oll thanowno SO, hars.\n",
            "Whind. HARER: Heceand me,\n",
            "Call swist meesesen:\n",
            "Whait the bot,\n",
            "Gean anen,\n",
            "Hoto art.\n",
            "And esit,\n",
            "thirshey ies w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple self-attention heads in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1) #results of each head concatenated together along the last axis (channel dimension)\n",
        "\n",
        "mtmp = MultiHeadAttention(4, int(n_embd/4) )"
      ],
      "metadata": {
        "id": "r_izZFAtAwgt"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, int(n_embd/4) ) #n_embd needs to be divisible by 4\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = MultiHeadSelfAttentionModel()\n",
        "logits, loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "RG_zFvApBjzX"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq-H0AAZFb8P",
        "outputId": "7079c7b7-8f6b-4848-d27e-cb86d03a6ec8"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 | train: 4.209168910980225 | test: 4.207609176635742\n",
            "iter 1000 | train: 2.483496904373169 | test: 2.48531436920166\n",
            "iter 2000 | train: 2.3885586261749268 | test: 2.382492780685425\n",
            "iter 3000 | train: 2.315964460372925 | test: 2.3427982330322266\n",
            "iter 4000 | train: 2.3008534908294678 | test: 2.310729742050171\n",
            "iter 5000 | train: 2.270538091659546 | test: 2.2876574993133545\n",
            "iter 6000 | train: 2.2408392429351807 | test: 2.261523485183716\n",
            "iter 7000 | train: 2.23309063911438 | test: 2.251828670501709\n",
            "iter 8000 | train: 2.2166357040405273 | test: 2.2336173057556152\n",
            "iter 9000 | train: 2.189791202545166 | test: 2.2387702465057373\n",
            "2.296988010406494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbNUT89yIvJo",
        "outputId": "cc224c7e-459f-421f-e2e4-2c8719495d56"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Be the ucer an; mut and verewarew'bay ware to de, het awill te to rue-or thalt fordar. OLIO:\n",
            "O yeved itfr cheild theis hat thare groabl port quyees, is thus is iscome red mark dit thenepisit thol ft,\n",
            "Nom,\n",
            "And liknea, the meyeacrecome.\n",
            "Buld manstay is inkiet.\n",
            "\n",
            "Thee.\n",
            "\n",
            "I JUCG RIDUCE ESou to bre to ay.\n",
            "\n",
            "LAp my,\n",
            "I east of thes\n",
            "Actut that shon, I noghtay: pe thongl\n",
            "Wellerp Rivet coownour sioprs.\n",
            "Whiand thild haveeand mior all swill meest.\n",
            "But ha\n",
            "terour of inean anet,\n",
            "Isto a the enoe it,\n",
            "thir hey lestw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\" just a linear layer and subsequent nonlinearity\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd,n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "PKxaswEwJHZt"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, int(n_embd/4) ) #n_embd needs to be divisible by 4\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffwd(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = MultiHeadSelfAttentionModel()\n",
        "logits, loss = m(xb,yb)"
      ],
      "metadata": {
        "id": "nXXOIHccJx_U"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdOuSmoAKBjO",
        "outputId": "6e343c37-faf6-4cc1-e9d3-407825458c4b"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 | train: 2.0813047885894775 | test: 2.1623427867889404\n",
            "iter 1000 | train: 2.0589511394500732 | test: 2.135934829711914\n",
            "iter 2000 | train: 2.084308385848999 | test: 2.1430416107177734\n",
            "iter 3000 | train: 2.0503218173980713 | test: 2.1528518199920654\n",
            "iter 4000 | train: 2.0690951347351074 | test: 2.1448447704315186\n",
            "iter 5000 | train: 2.0703284740448 | test: 2.153061628341675\n",
            "iter 6000 | train: 2.067626476287842 | test: 2.1493310928344727\n",
            "iter 7000 | train: 2.068558931350708 | test: 2.1481826305389404\n",
            "iter 8000 | train: 2.069783926010132 | test: 2.1434154510498047\n",
            "iter 9000 | train: 2.053410053253174 | test: 2.1550114154815674\n",
            "2.2344071865081787\n",
            "\n",
            "Bert mauck on, thou put veetwerew's envall thide, het anurth est life-or I the forsay. I lok to necontif lentild fayis hat thare warre sperang may, forter\n",
            "This is of in; umad;\n",
            "Wet thenepelit thol fould my worclanfed, the mode hat blusso kick asty a kin,\n",
            "Set.\n",
            "\n",
            "Thee.\n",
            "\n",
            "I stit!\n",
            "Maded es the pore to nath, ap my,\n",
            "Mor of of be slectuke I my son, I noghtak: pe nown lonenter to whercoombugh simbrine sin,\n",
            "This dard worch moorbald swill my seren:\n",
            "What brour of inen theen,\n",
            "Into art.\n",
            "ACUETLA:\n",
            "Breirshty impow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Transformer Model"
      ],
      "metadata": {
        "id": "dhnQ4U0NTXak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 64\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "nfj_Zn6FSrBJ"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Formalizing\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" One-headed self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # torch will recognize that 'tril' is not a model parameter\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape #batch, time, channel\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.query(x) #(B,T,C)\n",
        "    v = self.value(x) #(B,T,C)\n",
        "\n",
        "    wei = q @ k.swapaxes(-2,-1) * C**(-0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "\n",
        "    wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
        "    weu = self.dropout(wei)\n",
        "    out = wei @ v #(B,T,C)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple self-attention heads in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1) #results of each head concatenated together along the last axis (channel dimension)\n",
        "    out = self.dropout( self.proj(out) )\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" just a linear layer and subsequent nonlinearity\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential( \n",
        "        nn.Linear(n_embd, 4 * n_embd),    #the choice of 4 here is empirical\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication follwed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) #communication (matcing keys and queries among tokens to get values for each token)\n",
        "    self.ffwd = FeedForward(n_embd) #computation (operating on the token values)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x+self.sa( self.ln1(x) ) #residual connections\n",
        "    x = x+self.ffwd( self.ln2(x) )\n",
        "    return x"
      ],
      "metadata": {
        "id": "mHybGbU3Ldhk"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = MultiHeadSelfAttentionModel()\n",
        "print(f'{sum(p.numel() for p in m.parameters())} parameters' )\n",
        "m.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgqq3JYINWMB",
        "outputId": "6b783140-2df2-408b-ee90-9f72c3ca320c"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadSelfAttentionModel(\n",
              "  (token_embedding_table): Embedding(65, 128)\n",
              "  (position_embedding_table): Embedding(256, 128)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (1): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (2): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (3): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (1): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (2): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (3): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (1): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (2): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (3): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (1): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (2): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "          (3): Head(\n",
              "            (key): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (query): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (value): Linear(in_features=128, out_features=32, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "id": "XinBmNtqPaWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}