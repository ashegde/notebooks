{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+/j56SJn8L/DlLKtohr2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashegde/notebooks/blob/main/newton_schulz_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odXCww-LoXua"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In this notebook, we take a very simple look at the Rectangular Newton-Schulz algorithm\n",
        "developed in:\n",
        "\n",
        "Bernstein, J., & Newhouse, L. (2024). Modular Duality in Deep Learning. arXiv preprint arXiv:2410.21265.\n",
        "\n",
        "This paper, and several preceding works, e.g.,\n",
        "\n",
        "Bernstein, J., & Newhouse, L. (2024). Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325.\n",
        "\n",
        "and\n",
        "\n",
        "Yang, G., Simon, J. B., & Bernstein, J. (2023). A spectral condition for feature learning. arXiv preprint arXiv:2310.17813.\n",
        "\n",
        "provide a rather fascinating look at architecture-adapted optimization algorithms for neural networks. The overarching\n",
        "principal seems to be to ensure that the layer-wise weight matrices remain well-conditioned or well-normed at initialization\n",
        "and during training. This is accomplished by ensuring that the input and output vectors of each layer are appropriately sized,\n",
        "where \"size\" is measured by the corresponding vector space norms. The general rule is that in a d-dimensional vector space,\n",
        "vectors should be of size approximately \\sqrt{d}, which coincides with standard normalization schemes such as BatchNorm, LayerNorm, and RMSNorm.\n",
        "\n",
        "From my reading, this perspective seems quite distict from and complementary to other works in the field, which focus on the training\n",
        "dynamics of stochastic gradient methods and the associated implicit biases (e.g., towards low \"complexity\" solutions). There is a lot more\n",
        "to explore in this space.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "tuA5gFRWqMxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Newton_Schulz(A: np.ndarray, n_steps: int = 15, eps: float = 1e-9):\n",
        "  \"\"\"\n",
        "  Rectangular Newton-Schulz Iteration\n",
        "  \"\"\"\n",
        "  # normalize to ensure singular values of A are\n",
        "  # contained in between 0 and \\sqrt{3}.\n",
        "  X = A / (eps + np.linalg.norm(A, ord = 'fro'))\n",
        "  a = float(3/2)\n",
        "  b = float(1/2)\n",
        "  for _ in range(n_steps):\n",
        "      X = a * X - b * X @ X.T @ X\n",
        "  return X\n",
        "\n",
        "# Note, to see what the above iteration is doing, consider the SVD of X: X = USV'\n",
        "# The RHS breaks down into U(a*S - b*S^3)V', hence the polynomial in question\n",
        "# is applied directly to each singular value."
      ],
      "metadata": {
        "id": "KU0vuVxGqHg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "d = 500\n",
        "A = 1/np.sqrt(d) * np.random.randn(n,d)\n",
        "Ua, Sa, Vha = np.linalg.svd(A, full_matrices=False)"
      ],
      "metadata": {
        "id": "IWEWMlDsrWFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = Newton_Schulz(A)\n",
        "Ux, Sx, Vhx = np.linalg.svd(X, full_matrices=False)"
      ],
      "metadata": {
        "id": "LbqSP8M-r_4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error (how close is X to UV'?)\n",
        "rel_error = np.linalg.norm(X - Ua@Vha) / np.linalg.norm(Ua@Vha)\n",
        "print(rel_error)"
      ],
      "metadata": {
        "id": "SD2AHw_JtNo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "bins = np.linspace(min( np.min(Sa), np.min(Sx)), max( np.max(Sa), np.max(Sx)), 20)\n",
        "plt.hist(Sa, bins=bins, color ='b', alpha=0.5, label = 'Pre-NS singular values')\n",
        "plt.hist(Sx, bins=bins, color='r', alpha=0.5, label = 'Post-NS singular values')\n",
        "plt.xlabel('Singular values')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Singular value distributions')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hlxBpBbnuXrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}