{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a modification of part 4 of Andrej Karpathy's stellar Makemore tutorial (https://www.youtube.com/watch?v=q8SA3rM6ckI) in which we implement and backpropogate through layer normalization rather than batch normalization.\n"
      ],
      "metadata": {
        "id": "-yC091qfC5iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!git clone https://github.com/karpathy/makemore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS3T9u6MiQxp",
        "outputId": "7db44e0a-3339-47e6-a05d-9f01d217990c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'makemore'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 61 (delta 34), reused 43 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('makemore/names.txt','r') as file:\n",
        "  words = file.read().splitlines()"
      ],
      "metadata": {
        "id": "ZkGfCqJ1q_tD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_lengths = torch.tensor([len(w) for w in words]).float()\n",
        "print(\n",
        " f\"\"\"\n",
        " This dataset contains {word_lengths.nelement()} names\\n\n",
        " The minimum name length is {word_lengths.min()} characters.\\n \n",
        " The maximum name length is {word_lengths.max()} characters.\\n\n",
        " The mean name length is  {word_lengths.mean():.2f} characters. \\n\n",
        " The associated standard deviation is {word_lengths.std():.2f} characters.\n",
        " \"\"\"\n",
        " )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKCjx5iWrAKY",
        "outputId": "7c2fac4a-7286-4220-9d41-3fbf95d4c70a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " This dataset contains 32033 names\n",
            "\n",
            " The minimum name length is 2.0 characters.\n",
            " \n",
            " The maximum name length is 15.0 characters.\n",
            "\n",
            " The mean name length is  6.12 characters. \n",
            "\n",
            " The associated standard deviation is 1.44 characters.\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#building the character vocabulary and lookup tables to map from characters to integer indices and back\n",
        "\n",
        "chars = ['.']+sorted(list(set(''.join(words))))  #as before, '.' is used as a start/stop/padding special character\n",
        "s_to_i = {s:i for i,s in enumerate(chars)}\n",
        "i_to_s = {i:s for s,i in s_to_i.items()}\n",
        "block_size = 3 #context length, size of the block that supports the prediction: P(x_n| x_{n-1}, x_{n-2}, x_{n-3} )\n",
        "vocab_size = len(i_to_s)\n",
        "print(i_to_s)\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "id": "V1NQVFWDrGQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d884fe83-b550-40bf-dd2e-f4391677f6d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words, block_size):\n",
        "  X,Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    #print(w)\n",
        "    context = [0] * block_size #init context using indices of chars\n",
        "    for ch in w+'.':\n",
        "      ix = s_to_i[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      #print(''.join(i_to_s[i] for i in context), '--->', i_to_s[ix]) #context ---> current, training pattern\n",
        "      context = context[1:]+[ix]\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape,Y.shape)\n",
        "  return X,Y\n",
        "\n",
        "#training split (used to train parameters), dev/validation split (used to train hyperparameters), test split (at end with the final model)\n",
        "# 80%, 10%, 10%\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
        "Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
        "Xte, Yte = build_dataset(words[n2:], block_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48XyvM0RjqFJ",
        "outputId": "277bb448-a1dd-4e84-fa0b-81f66f0562f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#re-initialize all parameters using the Kaiming init method for tanh\n",
        "n_embd = 10 #embedding dimension\n",
        "n_hidden = 200\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size,n_embd),             generator=g) #embedding matrix\n",
        "W1 = torch.randn((n_embd * block_size,n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)  #scaling can be important, large weights that occur by chance (high dimensional gaussian) can cause the tanh nonlinearity to saturate, even at initialization. Saturated nonlinearities are flat, meaning the gradient of the loss wrt those parameters is zero. No learning for those parameters.\n",
        "\n",
        "# note that with batch normalization, the bias b1 is useless. We include it here anyway just to show it will have zero gradient.\n",
        "b1 = torch.randn(n_hidden,                       generator=g) * 0.1\n",
        "W2 = torch.randn((n_hidden,vocab_size),          generator=g) * 0.1 #scaling can help with unbalanced initial probabilities output by the softmax layer due to outliers in the random input layer\n",
        "b2 = torch.randn(vocab_size,                     generator=g) * 0.1\n",
        "\n",
        "#\n",
        "lngain = torch.randn((1,n_hidden))*0.1 + 1.0 #batch normalization gain\n",
        "lnbias = torch.randn((1,n_hidden))*0.1 #batch normalization bias\n",
        "\n",
        "#As stated in the lecture, the above initializations are somewhat nonstandard.\n",
        "#We are avoiding certain initializations, such as zero, so that improper implementations\n",
        "#of backprop will be fully exposed (nothing is hidden). \n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, lngain, lnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "batch_size = 32\n",
        "max_iters = 200000\n",
        "lossi=[]\n",
        "\n",
        "for i in range(max_iters):\n",
        "\n",
        "  ix = torch.randint(0,Xtr.shape[0],(batch_size,), generator=g) \n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] #batch\n",
        "\n",
        "  #forward pass\n",
        "  emb = C[Xb] #embedding characters (batch_size, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0],-1) #(batch_size, block_size * n_embd)\n",
        "\n",
        "  # Linear layer 1\n",
        "  hpreln = embcat @ W1 + b1 #h(idden)preb(atch)n(ormalization), size = (batch_size, n_hidden)\n",
        "  # each row of hprebn is a vector of preactivations for the corresponding input example.\n",
        "\n",
        "  # LayerNorm layer\n",
        "  lnmean = hpreln.mean(dim=1, keepdim=True)\n",
        "  lnvar = hpreln.var(dim=1, keepdim=True, unbiased=True)\n",
        "  lnstd_inv = (lnvar+1e-5)**-0.5\n",
        "  lnraw = (hpreln - lnmean) * lnstd_inv\n",
        "  hpreact = lngain * lnraw + lnbias #(batch_size, n_hidden)\n",
        "\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) #(batch_size, n_hidden)\n",
        "\n",
        "  #Linear layer 2\n",
        "  logits = h @ W2 + b2 #(batch_size, vocab_size)\n",
        "\n",
        "  #cross entropy loss, does the same thing as F.cross_entropy(logits,Yb)\n",
        "  loss = F.cross_entropy(logits, Yb) # negative log likelihood loss, averaged over the batch\n",
        "  \n",
        "  #backward pass\n",
        "\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "\n",
        "  loss.backward() \n",
        "\n",
        "  #update\n",
        "  lr = 0.1 if i < 100000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0:\n",
        "    print(f'{i:7d}/{max_iters:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i == 1000: #debug\n",
        "    #   break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_bdlUcXJijG",
        "outputId": "4a03481d-8ae7-4f28-f9c7-76f009a15132"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7656\n",
            "  10000/ 200000: 2.1812\n",
            "  20000/ 200000: 2.3081\n",
            "  30000/ 200000: 2.4482\n",
            "  40000/ 200000: 1.9748\n",
            "  50000/ 200000: 2.4055\n",
            "  60000/ 200000: 2.4535\n",
            "  70000/ 200000: 2.0327\n",
            "  80000/ 200000: 2.2801\n",
            "  90000/ 200000: 2.1518\n",
            " 100000/ 200000: 1.8942\n",
            " 110000/ 200000: 2.1645\n",
            " 120000/ 200000: 1.9131\n",
            " 130000/ 200000: 2.4799\n",
            " 140000/ 200000: 2.3318\n",
            " 150000/ 200000: 2.1241\n",
            " 160000/ 200000: 1.8702\n",
            " 170000/ 200000: 1.8250\n",
            " 180000/ 200000: 1.9836\n",
            " 190000/ 200000: 1.8101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  x,y = { \n",
        "      'train': (Xtr, Ytr),\n",
        "      'val': (Xdev,Ydev),\n",
        "      'test': (Xte,Yte),\n",
        "  }[split]\n",
        "  emb = C[x] #(N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0],-1)\n",
        "  hpreln = embcat @ W1 + b1\n",
        "  # layer norm\n",
        "  lnmean = hpreln.mean(dim=1, keepdim=True)\n",
        "  lnvar = hpreln.var(dim=1, keepdim=True, unbiased=True)\n",
        "  lnstd_inv = (lnvar+1e-5)**-0.5\n",
        "  lnraw = (hpreln - lnmean) * lnstd_inv\n",
        "  hpreact = lngain * lnraw + lnbias\n",
        "  #\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "  \n",
        "split_loss('train')\n",
        "split_loss('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Ny7UJrPjQ6",
        "outputId": "72b1a872-ba5d-429c-b178-79b2cf428300"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0661628246307373\n",
            "test 2.112459182739258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model with layer normalization has similar performance to batch normalization."
      ],
      "metadata": {
        "id": "TlZLCTzeeRxM"
      }
    }
  ]
}