{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashegde/notebooks/blob/main/nano_nGPT_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yC091qfC5iT"
      },
      "source": [
        "The material in this notebook follows Andrej Karpathy's nanoGPT tutorial (https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "Edit: 10/11/2024, modified to include the normalized transformer as described in:\n",
        "\n",
        "Loshchilov, I., Hsieh, C. P., Sun, S., & Ginsburg, B. (2024). nGPT: Normalized Transformer with Representation Learning on the Hypersphere. arXiv preprint arXiv:2410.01131.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS3T9u6MiQxp"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZYCx9z5Pioj"
      },
      "outputs": [],
      "source": [
        "#Loading the TinyShakespeare Dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osWkIVz2P3IK"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  dataset = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNIqZzXAP-Pc"
      },
      "outputs": [],
      "source": [
        "print(f'The dataset contains {len(dataset)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8IQA1snTfVd"
      },
      "outputs": [],
      "source": [
        "print(type(dataset)) #this dataset is just a big string (not a list of strings, we haven't used split or anything)\n",
        "print(dataset[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7LvCMHkTvCb"
      },
      "outputs": [],
      "source": [
        "# extract the unique characters/symbols/atoms that build the dataset\n",
        "vocab = sorted(list(set(dataset)))\n",
        "vocab_size = len(vocab)\n",
        "print(''.join(vocab))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiwneHFcUZBm"
      },
      "outputs": [],
      "source": [
        "# CHARACTER-LEVEL TOKENIZER\n",
        "\n",
        "class Tokenizer:\n",
        "  \"\"\"\n",
        "  Tokenizer based on an input character list\n",
        "\n",
        "  This class provides functionality for converting text\n",
        "  to character-level tokens.\n",
        "  \"\"\"\n",
        "  def __init__(self, unique_characters: str):\n",
        "    self.vocab = unique_characters\n",
        "    self.character_to_index = {ch:i for i,ch in enumerate(self.vocab)}\n",
        "    self.index_to_character = {i:ch for i,ch in enumerate(self.vocab)}\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Encode the input text into token indices\n",
        "\n",
        "    Args:\n",
        "      text (str): string to tokenize\n",
        "\n",
        "    Returns:\n",
        "      list: list of corresponding token indices\n",
        "    \"\"\"\n",
        "    return [self.character_to_index[c] for c in text]\n",
        "\n",
        "  def decode(self, indices: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decode the list of token indices into a string\n",
        "\n",
        "    Args:\n",
        "      indices (list[int]): list of integer token indices\n",
        "\n",
        "    Returns:\n",
        "      str: corresponding string\n",
        "    \"\"\"\n",
        "    return \"\".join([self.index_to_character[i] for i in indices])\n",
        "\n",
        "  def get_vocab_size(self):\n",
        "    \"\"\"Returns the size of the vocabulary\"\"\"\n",
        "    return len(self.vocab)\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "print(tokenizer.encode('hello there'))\n",
        "print(tokenizer.decode(tokenizer.encode('hello there')))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(\n",
        "    dataset: str,\n",
        "    tokenizer: Tokenizer,\n",
        "    train_frac: float = 0.9,\n",
        ") -> tuple[torch.tensor, torch.tensor]:\n",
        "  \"\"\"\n",
        "  Prepares dataset for model training\n",
        "\n",
        "  Converts the original dataset in the form of a string\n",
        "  into two sequences of token indices -- one for training\n",
        "  and one for testing.\n",
        "\n",
        "  Args:\n",
        "    dataset (str): dataset stored as a singled string\n",
        "    tokenizer (Tokenizer): tokenizer to encode the dataset\n",
        "    train_frac (float): fraction of dataset for training\n",
        "\n",
        "  Returns:\n",
        "    tuple(\n",
        "      train_data (torch.tensor): 1d tensor of token indices for training\n",
        "      val_data (torch.tensor): 1d tensor of token indices for validation\n",
        "    ):\n",
        "  \"\"\"\n",
        "  data = torch.tensor(\n",
        "      tokenizer.encode(dataset),\n",
        "      dtype=torch.long,\n",
        "  )\n",
        "\n",
        "  num_train = int(train_frac * len(data))\n",
        "  return (data[:num_train], data[num_train:])\n"
      ],
      "metadata": {
        "id": "qGjrbjkc3R2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  batch_size: int = 32\n",
        "  context_size: int = 256\n",
        "  max_iters: int = 5000\n",
        "  learning_rate: int = 3e-4\n",
        "  eval_iters: int = 200\n",
        "  vocab_size: int = 50257\n",
        "  n_layer: int = 6\n",
        "  n_head: int = 6\n",
        "  n_embd: int = 60\n",
        "  eps: float = 1e-6\n",
        "  device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = GPTConfig()"
      ],
      "metadata": {
        "id": "uhlPkIRd5jFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "4MIpV0B1I-lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umHgmoC1aP8K"
      },
      "outputs": [],
      "source": [
        "def get_batch(\n",
        "    data: torch.tensor,\n",
        "    config: GPTConfig\n",
        ") -> tuple[torch.tensor, torch.tensor]:\n",
        "  \"\"\"\n",
        "  Extracts a minibatch from the input dataset\n",
        "\n",
        "  Args:\n",
        "    data (torch.tensor): 1d tensor of token indices\n",
        "    config (Config): model settings config file\n",
        "\n",
        "  Returns:\n",
        "    tuple(\n",
        "      context (torch.tensor):\n",
        "      targets (torch.tensor):\n",
        "    )\n",
        "  \"\"\"\n",
        "  #select n random starting indices for a sequence of size block_size, where n = batch_size\n",
        "  context_size = config.context_size\n",
        "  batch_size = config.batch_size\n",
        "  device = config.device\n",
        "\n",
        "  ix = torch.randint(len(data)-context_size, (batch_size,))\n",
        "\n",
        "  context = torch.stack([data[i:i+context_size] for i in ix])  # (B,T)\n",
        "  targets = torch.stack([data[i+1:i+context_size+1] for i in ix])  # (B,T)\n",
        "  return context.to(device), targets.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = prepare_data(dataset, tokenizer, 0.9)\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "id": "aN8K6RvNFGCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trial: sample a minibatch\n",
        "xb, yb = get_batch(train_data, config)\n",
        "\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "# print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "# print(yb)\n",
        "\n",
        "# print('----')\n",
        "\n",
        "# #below we unpack all of the examples stored in each block in the batch\n",
        "# for b in range(config.batch_size): #b = batch\n",
        "#   for t in range(config.context_size): #t = time\n",
        "#     context = xb[b,:t+1]\n",
        "#     target = yb[b,t]\n",
        "#     print(f'When context is {context.tolist()}, the target is: {target}')"
      ],
      "metadata": {
        "id": "ERK3wxmI3lGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhnQ4U0NTXak"
      },
      "source": [
        "## Full Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHybGbU3Ldhk"
      },
      "outputs": [],
      "source": [
        "class L2Norm(nn.Module):\n",
        "  \"\"\"\n",
        "  L2 normalization layer\n",
        "  \"\"\"\n",
        "  def __init__(self, eps = 1e-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x, dim=-1):\n",
        "    # x is (..., D)\n",
        "    return x / (self.eps+torch.linalg.norm(x,dim=dim, keepdim=True))\n",
        "\n",
        "class NormableLinear(nn.Module):\n",
        "  \"\"\"\n",
        "  Linear module with normalizable rows.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dimension: int,\n",
        "      output_dimension: int,\n",
        "      scale: bool = False,\n",
        "      norm_dim: int = -1,\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.normable_weight = nn.Parameter(\n",
        "        1/math.sqrt(input_dimension) * torch.randn(output_dimension, input_dimension),\n",
        "    )\n",
        "    self.norm = L2Norm()\n",
        "    self.norm_dim = norm_dim\n",
        "    self.is_scale = scale\n",
        "    self.scale = nn.Parameter(\n",
        "        torch.zeros(output_dimension),\n",
        "    ) if self.is_scale else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is (..., input_dimension)\n",
        "    # returns x@W.T\n",
        "    if self.is_scale:\n",
        "      return x@self.normable_weight.transpose(0,1) * self.scale.exp()\n",
        "    else:\n",
        "      return x@self.normable_weight.transpose(0,1)\n",
        "\n",
        "  def get_indices(self, index: list[int]):\n",
        "    if self.is_scale:\n",
        "      return self.normable_weight[index] * self.scale[index].exp()\n",
        "    else:\n",
        "      return self.normable_weight[index]\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def normalize(self):\n",
        "    \"\"\"\n",
        "    Row-normalization of the weight matrix.\n",
        "    \"\"\"\n",
        "    self.normable_weight.copy_(\n",
        "        self.norm(self.normable_weight, dim=self.norm_dim),\n",
        "    )\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 # ensure divisibility for multiple heads\n",
        "        self.c_attn = NormableLinear(config.n_embd, 3 * config.n_embd, scale=True) #3 for the query, key, and value matrices\n",
        "        # output projection\n",
        "        self.c_proj = NormableLinear(config.n_embd, config.n_embd, scale=True)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is (B, T, C),\n",
        "        # [B]atches\n",
        "        # [T]okens := context_size = number of tokens in the sequence\n",
        "        # [C]hannels := n_embd = n_heads * h_size. Recall, h_size = n_embd // n_heads = C // n_heads\n",
        "\n",
        "        B, T, C = x.size()\n",
        "        nh = self.n_head\n",
        "        hs = C // self.n_head\n",
        "        qkv = self.c_attn(x) #(B, T, 3 * n_embd)\n",
        "\n",
        "        # split dim 2 into chunks of size n_embd --> in this case, we will have 3 chunks for query, key, and value outputs\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2) # q, k, v are each (B, T, n_embd)\n",
        "\n",
        "        # distribute the projections across different heads\n",
        "        q = q.view(B, T, nh, hs).transpose(1,2) #(B, n_head, T, h_size)\n",
        "        q = q * hs # to rescale the attention dot product\n",
        "        k = k.view(B, T, nh, hs).transpose(1,2) #(B, n_head, T, h_size)\n",
        "        v = v.view(B, T, nh, hs).transpose(1,2) #(B, n_head, T, h_size)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
        "\n",
        "        y = y.transpose(1,2).contiguous().view(B,T,C) #(B,T, C = n_heads * h_size)\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        NormableLinear(n_embd, 4*n_embd, scale=True),    #the choice of 4 here is empirical\n",
        "        nn.GELU(),\n",
        "        NormableLinear(4*n_embd, n_embd, scale=True, norm_dim=0),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    n_embd = config.n_embd\n",
        "    n_head = config.n_head\n",
        "    self.causal_attn = CausalSelfAttention(config)\n",
        "    self.mlp = MLP(config.n_embd)\n",
        "    self.norm = L2Norm()\n",
        "\n",
        "\n",
        "    self.scale_attn = nn.Parameter(-0.5*math.log(n_embd)*torch.ones(n_embd))\n",
        "    self.scale_mlp = nn.Parameter(-0.5*math.log(n_embd)*torch.ones(n_embd))\n",
        "\n",
        "  def forward(self, h):\n",
        "    hA = self.norm(self.causal_attn(h))\n",
        "    hM = self.norm(self.mlp(h))\n",
        "    h = self.norm(h + self.scale_attn.exp() * (hA - h))\n",
        "    h = self.norm(h + self.scale_mlp.exp() * (hM - h))\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgqq3JYINWMB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.context_size = config.context_size\n",
        "    self.token_embedding_table = NormableLinear(config.n_embd, config.vocab_size)\n",
        "    self.position_embedding_table = NormableLinear(config.n_embd, config.context_size)\n",
        "    self.norm = L2Norm()\n",
        "    self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "    self.lm_head = NormableLinear(config.n_embd, config.vocab_size, scale = True)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size\n",
        "    # idx and targets are both integer tensors of dimension (B,T)\n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table.get_indices(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table.get_indices(torch.arange(T, device=config.device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.norm(x)\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def normalize(self):\n",
        "    for name, module in model.named_modules():\n",
        "      if isinstance(module, NormableLinear):\n",
        "          module.normalize()\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens)\n",
        "      idx_cond = idx[:,-self.context_size:]\n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiHeadSelfAttentionModel(config)\n",
        "model.to(config.device)"
      ],
      "metadata": {
        "id": "aGqy-ahmKLqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.normalize()\n",
        "with torch.no_grad():\n",
        "  print(torch.linalg.norm(model.token_embedding_table.normable_weight[0]))"
      ],
      "metadata": {
        "id": "t45ZYG20Xny3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'This model has {sum(p.numel() for p in model.parameters())} parameters')"
      ],
      "metadata": {
        "id": "HC4msBBExvTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper functions\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(\n",
        "    model,\n",
        "    data,\n",
        "    config,\n",
        "):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  losses = torch.zeros(config.eval_iters)\n",
        "  for k in range(config.eval_iters):\n",
        "    X, Y = get_batch(data, config)\n",
        "    logits, loss = model(X,Y)\n",
        "    losses[k] = loss.item()\n",
        "\n",
        "  model.train()\n",
        "  return losses.mean()"
      ],
      "metadata": {
        "id": "4yCpQfnzHvuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XinBmNtqPaWp"
      },
      "outputs": [],
      "source": [
        "config.learning_rate = 3e-2\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.0)\n",
        "\n",
        "# for simplicity, we will just use an iteration-based scheduler (as opposed to epoch-based)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=100)\n",
        "\n",
        "print(f'Device: {config.device}')\n",
        "for step in range(config.max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch(train_data, config)\n",
        "\n",
        "  # train\n",
        "  model.train()\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  logits, loss = model(xb,yb)\n",
        "\n",
        "  #loss\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  model.normalize()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    train_loss = estimate_loss(\n",
        "        model,\n",
        "        train_data,\n",
        "        config,\n",
        "    )\n",
        "    val_loss = estimate_loss(\n",
        "        model,\n",
        "        val_data,\n",
        "        config,\n",
        "    )\n",
        "    print(f'iter {step} | train: {train_loss} | test: {val_loss}')\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VCftNpO1U3gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "idx = idx.to(config.device)\n",
        "print( tokenizer.decode( model.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "id": "6SLA5awvzNKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVEz8jSvKZYsblKNeVrz5m",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}