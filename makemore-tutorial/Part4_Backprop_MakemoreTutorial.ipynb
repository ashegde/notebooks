{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The material in this notebook follows part 4 of Andrej Karpathy's stellar Makemore tutorial (https://www.youtube.com/watch?v=q8SA3rM6ckI)\n"
      ],
      "metadata": {
        "id": "-yC091qfC5iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!git clone https://github.com/karpathy/makemore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS3T9u6MiQxp",
        "outputId": "9934b4b7-589e-4b79-f954-0aa9f1c2dd7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'makemore' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('makemore/names.txt','r') as file:\n",
        "  words = file.read().splitlines()"
      ],
      "metadata": {
        "id": "ZkGfCqJ1q_tD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_lengths = torch.tensor([len(w) for w in words]).float()\n",
        "print(\n",
        " f\"\"\"\n",
        " This dataset contains {word_lengths.nelement()} names\\n\n",
        " The minimum name length is {word_lengths.min()} characters.\\n \n",
        " The maximum name length is {word_lengths.max()} characters.\\n\n",
        " The mean name length is  {word_lengths.mean():.2f} characters. \\n\n",
        " The associated standard deviation is {word_lengths.std():.2f} characters.\n",
        " \"\"\"\n",
        " )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKCjx5iWrAKY",
        "outputId": "feb395a3-41fb-4dbb-8818-613bc44b6b78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " This dataset contains 32033 names\n",
            "\n",
            " The minimum name length is 2.0 characters.\n",
            " \n",
            " The maximum name length is 15.0 characters.\n",
            "\n",
            " The mean name length is  6.12 characters. \n",
            "\n",
            " The associated standard deviation is 1.44 characters.\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#building the character vocabulary and lookup tables to map from characters to integer indices and back\n",
        "\n",
        "chars = ['.']+sorted(list(set(''.join(words))))  #as before, '.' is used as a start/stop/padding special character\n",
        "s_to_i = {s:i for i,s in enumerate(chars)}\n",
        "i_to_s = {i:s for s,i in s_to_i.items()}\n",
        "block_size = 3 #context length, size of the block that supports the prediction: P(x_n| x_{n-1}, x_{n-2}, x_{n-3} )\n",
        "vocab_size = len(i_to_s)\n",
        "print(i_to_s)\n",
        "print(vocab_size)\n"
      ],
      "metadata": {
        "id": "V1NQVFWDrGQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dc25c3-2ff7-4ebf-99af-e7c05d61b22c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(words, block_size):\n",
        "  X,Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    #print(w)\n",
        "    context = [0] * block_size #init context using indices of chars\n",
        "    for ch in w+'.':\n",
        "      ix = s_to_i[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      #print(''.join(i_to_s[i] for i in context), '--->', i_to_s[ix]) #context ---> current, training pattern\n",
        "      context = context[1:]+[ix]\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape,Y.shape)\n",
        "  return X,Y\n",
        "\n",
        "#training split (used to train parameters), dev/validation split (used to train hyperparameters), test split (at end with the final model)\n",
        "# 80%, 10%, 10%\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1], block_size)\n",
        "Xdev, Ydev = build_dataset(words[n1:n2], block_size)\n",
        "Xte, Yte = build_dataset(words[n2:], block_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48XyvM0RjqFJ",
        "outputId": "eccb6444-b41d-475c-aace-fec68e25b752"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference gradients for comparison to validate our manual gradients\n",
        "def cmp(s,dt,t):\n",
        "  ex = torch.all(dt == t.grad).item() #all entries exact match?\n",
        "  app = torch.allclose(dt, t.grad) #all entries close within some tolerance?\n",
        "  maxdiff = (dt - t.grad).abs().max().item() #max error?\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n"
      ],
      "metadata": {
        "id": "P-Jn60N1gliw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#re-initialize all parameters using the Kaiming init method for tanh\n",
        "n_embd = 10 #embedding dimension\n",
        "n_hidden = 64\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size,n_embd),             generator=g) #embedding matrix\n",
        "W1 = torch.randn((n_embd * block_size,n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)  #scaling can be important, large weights that occur by chance (high dimensional gaussian) can cause the tanh nonlinearity to saturate, even at initialization. Saturated nonlinearities are flat, meaning the gradient of the loss wrt those parameters is zero. No learning for those parameters.\n",
        "\n",
        "# note that with batch normalization, the bias b1 is useless. We include it here anyway just to show it will have zero gradient.\n",
        "b1 = torch.randn(n_hidden,                       generator=g) * 0.1\n",
        "W2 = torch.randn((n_hidden,vocab_size),          generator=g) * 0.1 #scaling can help with unbalanced initial probabilities output by the softmax layer due to outliers in the random input layer\n",
        "b2 = torch.randn(vocab_size,                     generator=g) * 0.1\n",
        "\n",
        "#\n",
        "bngain = torch.randn((1,n_hidden))*0.1 + 1.0 #batch normalization gain\n",
        "bnbias = torch.randn((1,n_hidden))*0.1 #batch normalization bias\n",
        "bnmean_running = torch.zeros((1, n_hidden))\n",
        "bnstd_running = torch.ones((1,n_hidden))\n",
        "\n",
        "#As stated in the lecture, the above initializations are somewhat nonstandard.\n",
        "#We are avoiding certain initializations, such as zero, so that improper implementations\n",
        "#of backprop will be fully exposed (nothing is hidden). \n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "W7ER4Qny43Oe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3c9361-e89a-4894-c465-3bce69579067"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed forward pass\n",
        "Exploring the guts of a single forward pass through the network"
      ],
      "metadata": {
        "id": "NCexElWAkdfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#construct minibatch indices\n",
        "batch_size = 32\n",
        "ix = torch.randint(0,Xtr.shape[0],(batch_size,), generator=g) \n",
        "Xb, Yb = Xtr[ix], Ytr[ix] #batch\n",
        "\n",
        "print(Xb.shape,Yb.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHXncndJ5H9k",
        "outputId": "9c76ea99-2699-4884-d5b7-9fbaeb2ad8e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3]) torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "# recall: \n",
        "# C is (vocab_size,n_embd)\n",
        "# Xb is (batch_size,block_size) \n",
        "\n",
        "\n",
        "emb = C[Xb] #embedding characters (batch_size, block_size, n_embd)\n",
        "embcat = emb.view(emb.shape[0],-1) #(batch_size, block_size * n_embd)\n",
        "\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 #h(idden)preb(atch)n(ormalization), size = (batch_size, n_hidden)\n",
        "# each row of hprebn is a vector of preactivations for the corresponding input example.\n",
        "\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/batch_size * hprebn.sum(dim=0,keepdim=True) #(1,n_hidden), batch average for each preactivation\n",
        "bndiff = hprebn - bnmeani #(batch_size, n_hidden)\n",
        "bndiff2 = bndiff**2\n",
        "bnvar= 1/(batch_size-1) * bndiff2.sum(dim=0,keepdim=True) #Bessel correction to sample variance\n",
        "bnstd_inv = (bnvar+1e-5)**-0.5 #1/bnstd_dev\n",
        "bnraw = bndiff * bnstd_inv # normalization (hprebn - bnmean) / bnstd_dev\n",
        "hpreact = bngain * bnraw + bnbias #(batch_size, n_hidden)\n",
        "\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) #(batch_size, n_hidden)\n",
        "\n",
        "#Linear layer 2\n",
        "logits = h @ W2 + b2 #(batch_size, vocab_size)\n",
        "\n",
        "#cross entropy loss, does the same thing as F.cross_entropy(logits,Yb)\n",
        "logit_maxes = logits.max(1, keepdim=True).values #calculates max of each output layer of each example, size=(batch_size,1)\n",
        "norm_logits = logits - logit_maxes #subtracts the max element from each output layer/row, size=(batch_size,vocab_size)\n",
        "counts = norm_logits.exp() #(batch_size, vocab_size)\n",
        "counts_sum = counts.sum(1,keepdim=True) #(batch_size,1)\n",
        "counts_sum_inv = counts_sum**-1\n",
        "probs = counts * counts_sum_inv #(batch_size, vocab_size)\n",
        "logprobs = probs.log() #k-th row contains the log likelihood distribution over all next tokens for the k-th example in the batch\n",
        "loss = -logprobs[range(batch_size),Yb].mean() # negative log likelihood loss, averaged over the batch\n",
        "\n",
        "\n",
        "#PyTorch backward pass\n",
        "#backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "\n",
        "for t in [logprobs,probs,counts,counts_sum, counts_sum_inv,\n",
        "         norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnstd_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "id": "6Uet_SIPktYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4067f2-f6a4-449e-cd74-90d8c501840a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3420, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Exercise 1: backprop through the whole thing manually, one-by-one.\n",
        "# notation dg is shorthand for d(loss)/dg\n",
        "\n",
        "#d(loss)/d(logprobs)\n",
        "# For example k, logprobs is a (1,vocab_size) sized log probability distribution over the vocab for the next words.\n",
        "# Hence, dlogprobs is of dimension (batch_size,vocab_size)\n",
        "# The negative log likelihood loss only plucks out the -logprob associated with Yb[k]. \n",
        "# Hence, this index is the only parameter in logprob that influences the loss.\n",
        "# This is averaged over the batch, hence each score has a coefficient of 1/batch_size.\n",
        "# The dimension of dlogprobs should be (batch_size,vocab_size) \n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs) \n",
        "dlogprobs[range(batch_size),Yb] = -1/batch_size\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "#d(loss)/d(probs) \n",
        "# For a fixed example k, logprobs is a (1,vocab_size) sized log probability distribution.\n",
        "# Each entry of is the natural logarithm of the corresponding entry of probs.\n",
        "# Hence, the dimension of dprobs is (batch_size, vocab_size)\n",
        "\n",
        "dprobs = probs**-1.0 * dlogprobs \n",
        "cmp('probs', dprobs, probs)\n",
        "\n",
        "#d(loss)/d(counts_sum_inv)\n",
        "# For a fixed example k, counts_sum_inv is a (1,1) sized parameter.\n",
        "# And for the batch of examples, counts_sum_inv is (batch_size,1)\n",
        "# Hence, d(probs)/d(counts_sum_inv) should be (batch_size,vocab_size).\n",
        "# This tensor contains all of the linear perturbations to the probabilities due to perturbations in counts_sum_inv.\n",
        "# From above, we have that d(loss)/d(probs) is of dimension (batch_size, vocab_size).\n",
        "# For each example, we can sum up all of the linear perturbations to the loss due to the perturbations to counts_sum_inv.\n",
        "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "\n",
        "#d(loss)/d(counts_sum)\n",
        "# For a fixed example k, counts_sum is a (1,1) sized parameter.\n",
        "# And for the batch of examples, counts_sum is (batch_size,1)\n",
        "# Hence, d(count_sum_inv)/d(counts) should be (batch_size,1).\n",
        "# From above, we have that d(loss)/d(count_sum_inv)) is of dimension (batch_size, vocab_size).\n",
        "# For each example, we can sum up all of the linear perturbations to the loss due to the perturbations to counts_sum_inv.\n",
        "dcounts_sum = (-counts_sum**-2)  * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "#d(loss)/d(counts)\n",
        "# First, note that counts appears in two terms: probs and counts_sum .\n",
        "# Therefore, we must sum up the derivatives of these two terms (product rule).\n",
        "dcounts = counts_sum_inv * dprobs + torch.ones_like(counts) * dcounts_sum\n",
        "cmp('counts', dcounts, counts)\n",
        "\n",
        "#d(loss)/d(norm_logits)\n",
        "# norm_logits is (batch_size, vocab_size)\n",
        "dnorm_logits = counts * dcounts\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "\n",
        "#d(loss)/d(logit_maxes)\n",
        "# We should expect this derivative to be effectively zero since the normalization of the cross-entropy loss is included\n",
        "# only for numerical stability.\n",
        "dlogit_maxes = (-dnorm_logits).sum(dim=1, keepdim=True)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "\n",
        "#d(loss)/d(logits)\n",
        "# logits appears in both the expressions for norm_logits and logits_max.\n",
        "# recall, logits is (batch_size, vocab_size)\n",
        "arg_idx = torch.zeros_like(logits)\n",
        "arg_idx[range(batch_size), logits.argmax(dim=1)] = 1.0\n",
        "dlogits = dnorm_logits.clone() + arg_idx*dlogit_maxes # note, we took care of the negative sign when computing dlogit_maxes\n",
        "cmp('logits', dlogits, logits)\n",
        "\n",
        "#d(loss)/dh\n",
        "# recall, the h is of dimension (batch_size, n_hidden).\n",
        "# also recall that W2 is of dimension (n_hidden,vocab_size)\n",
        "dh = dlogits @ W2.T\n",
        "cmp('h', dh, h)\n",
        "\n",
        "#d(loss)/d(W2)\n",
        "# recall, the weight matrix W2 is of dimension (n_hidden, vocab_size)\n",
        "# h is (batch_size, n_hidden)\n",
        "# dlogits is (batch_size, vocab_size)\n",
        "dW2 = h.T @ dlogits\n",
        "cmp('W2', dW2, W2)\n",
        "\n",
        "#d(loss)/d(b2)\n",
        "# recall, the bias vector b2 is of dimension (vocab_size,) but will be broadcast to (1,vocab_size during operations)\n",
        "# h is (batch_size, n_hidden)\n",
        "# dlogits is (batch_size, vocab_size)\n",
        "db2 = dlogits.sum(dim=0)\n",
        "cmp('b2', db2, b2)\n",
        "\n",
        "#d(loss)/d(hpreact)\n",
        "# recall, the the preactivation layer in hpreact is (batch_size, n_hidden).\n",
        "# After the application of tanh, the resulting activation layer of course has the same dimension.\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "\n",
        "#d(loss)/d(bngain)\n",
        "# recall, bngain is of dimension (1,n_hidden).\n",
        "# After the application of tanh, the resulting activation layer of course has the same dimension.\n",
        "dbngain = (bnraw*dhpreact).sum(dim=0,keepdim=True)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "\n",
        "#d(loss)/d(bnraw)\n",
        "#recall, bnraw is of dimension (batch_size, n_hidden)\n",
        "dbnraw = bngain * dhpreact\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "\n",
        "#d(loss)/d(bnbias)\n",
        "#recall, bnbias is of dimension (1, n_hidden)\n",
        "dbnbias = dhpreact.sum(dim=0,keepdim=True)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "\n",
        "#d(loss)/d(bnstd_inv)\n",
        "#recall, bnstd_inv is of dimension (1, n_hidden)\n",
        "dbnstd_inv = (bndiff*dbnraw).sum(dim=0, keepdim=True)\n",
        "cmp('bnstd_inv', dbnstd_inv, bnstd_inv)\n",
        "\n",
        "#d(loss)/d(bnvar)\n",
        "#recall, bnvar is of dimension (1, n_hidden)\n",
        "dbnvar= -0.5*(bnvar+1e-5)**-1.5 *dbnstd_inv\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "\n",
        "#d(loss)/d(bndiff2)\n",
        "#recall, bndiff2 is of dimension (batch_size, n_hidden)\n",
        "dbndiff2 = 1/(batch_size-1)*torch.ones_like(bndiff2)*dbnvar\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "\n",
        "#d(loss)/d(bndiff)\n",
        "#recall, bndiff is of dimension (batch_size, n_hidden)\n",
        "dbndiff = bnstd_inv * dbnraw + 2.0 * bndiff * dbndiff2\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "\n",
        "#d(loss)/d(bnmeani)\n",
        "#recall, bnmeani is of dimension (1,n_hidden)\n",
        "dbnmeani= -1.0 * dbndiff.sum(dim=0, keepdim=True)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "\n",
        "#d(loss)/d(hprebn)\n",
        "#recall, hprebn is of dimension (batch_size, n_hidden)\n",
        "dhprebn = dbndiff.clone() + 1.0/batch_size*torch.ones_like(hprebn) *dbnmeani\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "\n",
        "#d(loss)/d(W1)\n",
        "#recall, W1 is of dimension (block_size * n_embed, n_hidden)\n",
        "dW1 = embcat.T @ dhprebn\n",
        "cmp('W1', dW1, W1)\n",
        "\n",
        "#d(loss)/d(b1)\n",
        "#recall, b1 is of dimension (n_hidden,)\n",
        "db1 = dhprebn.sum(dim=0)\n",
        "cmp('b1', db1, b1)\n",
        "\n",
        "#d(loss)/d(embcat)\n",
        "#recall embcat is of dimension (batch_size, block_size * n_embed)\n",
        "dembcat = dhprebn@W1.T\n",
        "cmp('embcat', dembcat, embcat)\n",
        "\n",
        "#d(loss)/d(emb)\n",
        "demb = dembcat.view(emb.shape)\n",
        "cmp('emb', demb, emb)\n",
        "\n",
        "#d(loss)/d(C)\n",
        "#recall, C is of dimension (vocab_size, n_embed)\n",
        "dC = torch.zeros_like(C)\n",
        "for ii in range(Xb.shape[0]):\n",
        "  for jj in range(Xb.shape[1]):\n",
        "    dC[Xb[ii,jj]] += demb[ii,jj,:]\n",
        "cmp('C', dC, C)\n",
        "\n",
        "# I seem to get mostly approximate equivalence whereas the lecture has exact equivalence for all derivatives. Perhaps I have a bug somewhere..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-QC1ifgYE9g",
        "outputId": "7d163c84-7aab-482d-ff57-ce774e4ceb89"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnstd_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.1827872842550278e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through the cross_entropy loss again, but this time all in one go rather than tracking each individual step.\n",
        "\n",
        "# Basically, the long way -- as investigated in the previous cell -- had the following forward pass:\n",
        "\n",
        "# logit_maxes = logits.max(1, keepdim=True).values #calculates max of each output layer of each example, size=(batch_size,1)\n",
        "# norm_logits = logits - logit_maxes #subtracts the max element from each output layer/row, size=(batch_size,vocab_size)\n",
        "# counts = norm_logits.exp() #(batch_size, vocab_size)\n",
        "# counts_sum = counts.sum(1,keepdim=True) #(batch_size,1)\n",
        "# counts_sum_inv = counts_sum**-1\n",
        "# probs = counts * counts_sum_inv #(batch_size, vocab_size)\n",
        "# logprobs = probs.log() #k-th row contains the log likelihood distribution over all next tokens for the k-th example in the batch\n",
        "# loss = -logprobs[range(batch_size),Yb].mean() # negative log likelihood loss, averaged over the batch\n",
        "\n",
        "# now, just consider:\n",
        "loss_fast = F.cross_entropy(logits,Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "id": "tKXiMoftroJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba74bb11-5939-4799-990e-7c0a64cec1ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.342024564743042 diff: -2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#backward pass\n",
        "# here we implement an analytical formula for d(loss)/dlogits\n",
        "\n",
        "# first, let us recall that logits is of dimension (batch_size, vocab_size).\n",
        "# now, let us consider the case of a single example rather than a minibatch.\n",
        "# the model output in terms of logits z is of dimension (vocab_size,).\n",
        "# this can be converted to probabilities by applying a softmax operator to get:\n",
        "#               p_i = exp(z_i) / $_j exp(z_j)\n",
        "# which is stored in the vector p of dimension (vocab_size,).\n",
        "# the nll loss is therefore -log p_y, where y is the index of target training data.\n",
        "#\n",
        "# hence, we are interested in \n",
        "# d/dz_k (-log p_y) = -1/p_y dp_y/dz_k\n",
        "#                   = -1/p_y d/dz_k (exp(z_y) / $_j exp(z_j)) \n",
        "#                   = -1/p_y [( ($_j exp(z_j)) * d(exp(z_y))/dz_k - exp(z_y)*d($_j exp(z_j))/dz_k) ) / ($_j exp(z_j))**2 ] \n",
        "#                   = -1/p_y [( ($_j exp(z_j)) * d(exp(z_y))/dz_k - exp(z_y)*exp(z_k)) ) / ($_j exp(z_j))**2 ] \n",
        "#                   = -1/p_y [ ($_j exp(z_j)) * d(exp(z_y))/dz_k / ($_j exp(z_j))**2 - exp(z_y)*exp(z_k)) ) / ($_j exp(z_j))**2 ] \n",
        "#                   = -1/p_y [  d(exp(z_y))/dz_k / ($_j exp(z_j)) - exp(z_y)*exp(z_k)) ) / ($_j exp(z_j))**2 ] \n",
        "#                   = -1/p_y [  p_y * 1_{k==y} - p_y p_k ]\n",
        "#                   = - [ 1_{k==y} - p_k ]\n",
        "#                   = p_k - 1_{k==y}\n",
        "# now recall our loss is averaged over a minibatch.\n",
        "\n",
        "#dlogits_ = 1/batch_size * (probs - F.one_hot(Yb, num_classes=vocab_size) )\n",
        "dlogits_ = F.softmax(logits, dim=1)\n",
        "dlogits_[range(batch_size),Yb] -= 1\n",
        "dlogits_ *= 1/batch_size\n",
        "cmp('logits', dlogits_, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vav2Kn9Y3iKG",
        "outputId": "5f192efb-a379-4e96-a97b-ac7819288395"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.122274160385132e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An interpretation of the negative log marginal likelihood derivatives with respect to the logits is as follows. Recall, the logits are soft maxed into a probability distribution $p$ over the `vocab_size` classes. Let $k$ index those classes and let $y$ by the index of the observed training data point.\n",
        "\n",
        "If $k \\neq y$, i.e., the $k$-th class does not correspond to the correct data label, then\n",
        "\n",
        "`d(loss)/d(logits[k]) = probs[k]`.\n",
        "\n",
        "Hence, further decreasing the loss via gradient descent pulls down the $k$-th logit.\n",
        "\n",
        "Conversely, if $k = y$, i.e., the $k$-th class is the correct data label, then\n",
        "\n",
        "`d(loss)/d(logits[k]) = probs[k] - 1 < 0`\n",
        "\n",
        "Therefore, further decreasing the loss via gradient descent pushes up the $k$-th logit.\n",
        "\n",
        "Basically, the output probabilities are adjusted to support the training data labels (while opposing incorrect labels). "
      ],
      "metadata": {
        "id": "elg5fO3eITzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through the batch normalization again, but this time all in one go.\n",
        "\n",
        "# recall, the forward pass:\n",
        "\n",
        "# bnmeani = 1/batch_size * hprebn.sum(dim=0,keepdim=True) #(1,n_hidden), batch average for each preactivation\n",
        "# bndiff = hprebn - bnmeani #(batch_size, n_hidden)\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar= 1/(batch_size-1) * bndiff2.sum(dim=0,keepdim=True) #Bessel correction to sample variance\n",
        "# bnstd_inv = (bnvar+1e-5)**-0.5 #1/bnstd_dev\n",
        "# bnraw = bndiff * bnstd_inv # normalization (hprebn - bnmean) / bnstd_dev\n",
        "# hpreact = bngain * bnraw + bnbias #(batch_size, n_hidden)\n",
        "\n",
        "# now, using one line:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(dim=0,keepdim=True)) / torch.sqrt(hprebn.var(dim=0, keepdim=True, unbiased = True) + 1e-5) + bnbias\n",
        "print('maxdiff: ', (hpreact_fast - hpreact).abs().max())\n"
      ],
      "metadata": {
        "id": "0qc7jwJAoaik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20f5c8e-1b70-4297-b133-5b4becedbf9b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxdiff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# our goal here is to calculate the d(loss)/d(hprebn) given d(loss)/d(hpreact). \n",
        "# from the analytical formula, we have: \n",
        "\n",
        "dhprebn_ = bngain * bnstd_inv / batch_size * ( batch_size*dhpreact - dhpreact.sum(dim=0) - batch_size / (batch_size-1) * bnraw *(dhpreact * bnraw).sum(dim=0))\n",
        "\n",
        "cmp('hprebn', dhprebn_, hprebn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAKc4zkPHGSV",
        "outputId": "8142cdc8-51da-44a1-9a1b-b997fe1504ea"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together"
      ],
      "metadata": {
        "id": "N6WeOJWLJgA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#re-initialize all parameters using the Kaiming init method for tanh\n",
        "n_embd = 10 #embedding dimension\n",
        "n_hidden = 200\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size,n_embd),             generator=g) #embedding matrix\n",
        "W1 = torch.randn((n_embd * block_size,n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)  #scaling can be important, large weights that occur by chance (high dimensional gaussian) can cause the tanh nonlinearity to saturate, even at initialization. Saturated nonlinearities are flat, meaning the gradient of the loss wrt those parameters is zero. No learning for those parameters.\n",
        "\n",
        "# note that with batch normalization, the bias b1 is useless. We include it here anyway just to show it will have zero gradient.\n",
        "b1 = torch.randn(n_hidden,                       generator=g) * 0.1\n",
        "W2 = torch.randn((n_hidden,vocab_size),          generator=g) * 0.1 #scaling can help with unbalanced initial probabilities output by the softmax layer due to outliers in the random input layer\n",
        "b2 = torch.randn(vocab_size,                     generator=g) * 0.1\n",
        "\n",
        "#\n",
        "bngain = torch.randn((1,n_hidden))*0.1 + 1.0 #batch normalization gain\n",
        "bnbias = torch.randn((1,n_hidden))*0.1 #batch normalization bias\n",
        "\n",
        "#As stated in the lecture, the above initializations are somewhat nonstandard.\n",
        "#We are avoiding certain initializations, such as zero, so that improper implementations\n",
        "#of backprop will be fully exposed (nothing is hidden). \n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters))\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "batch_size = 32\n",
        "max_iters = 200000\n",
        "lossi=[]\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(max_iters):\n",
        "\n",
        "    ix = torch.randint(0,Xtr.shape[0],(batch_size,), generator=g) \n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] #batch\n",
        "\n",
        "    #forward pass\n",
        "    emb = C[Xb] #embedding characters (batch_size, block_size, n_embd)\n",
        "    embcat = emb.view(emb.shape[0],-1) #(batch_size, block_size * n_embd)\n",
        "\n",
        "    # Linear layer 1\n",
        "    hprebn = embcat @ W1 + b1 #h(idden)preb(atch)n(ormalization), size = (batch_size, n_hidden)\n",
        "    # each row of hprebn is a vector of preactivations for the corresponding input example.\n",
        "\n",
        "    # BatchNorm layer\n",
        "    bnmean = hprebn.mean(dim=0, keepdim=True)\n",
        "    bnvar = hprebn.var(dim=0, keepdim=True, unbiased=True)\n",
        "    bnstd_inv = (bnvar+1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnstd_inv\n",
        "    hpreact = bngain * bnraw + bnbias #(batch_size, n_hidden)\n",
        "\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) #(batch_size, n_hidden)\n",
        "\n",
        "    #Linear layer 2\n",
        "    logits = h @ W2 + b2 #(batch_size, vocab_size)\n",
        "\n",
        "    #cross entropy loss, does the same thing as F.cross_entropy(logits,Yb)\n",
        "    loss = F.cross_entropy(logits, Yb) # negative log likelihood loss, averaged over the batch\n",
        "\n",
        "\n",
        "    #PyTorch backward pass\n",
        "    #backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "\n",
        "    #loss.backward() #for debug\n",
        "    # Manual backprop:\n",
        "\n",
        "    dlogits = F.softmax(logits, dim=1)\n",
        "    dlogits[range(batch_size),Yb] -= 1\n",
        "    dlogits /= batch_size\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(dim=0)\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    dbngain = (bnraw*dhpreact).sum(dim=0,keepdim=True)\n",
        "    dbnbias = dhpreact.sum(dim=0,keepdim=True)\n",
        "    dhprebn = bngain * bnstd_inv / batch_size * ( batch_size*dhpreact - dhpreact.sum(dim=0) - batch_size / (batch_size-1) * bnraw *(dhpreact * bnraw).sum(dim=0))\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(dim=0)\n",
        "    dembcat = dhprebn@W1.T\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for ii in range(Xb.shape[0]):\n",
        "      for jj in range(Xb.shape[1]):\n",
        "        dC[Xb[ii,jj]] += demb[ii,jj,:]\n",
        "\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "\n",
        "    #update\n",
        "    lr = 0.1 if i < 100000 else 0.01\n",
        "    for p, grad in zip(parameters,grads):\n",
        "      p.data += -lr * grad\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0:\n",
        "      print(f'{i:7d}/{max_iters:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i == 1000: #debug\n",
        "    #   break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_bdlUcXJijG",
        "outputId": "fc3e7fcf-a50e-49b0-99cc-948a8a212db2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7805\n",
            "  10000/ 200000: 2.1499\n",
            "  20000/ 200000: 2.3771\n",
            "  30000/ 200000: 2.4338\n",
            "  40000/ 200000: 1.9939\n",
            "  50000/ 200000: 2.4139\n",
            "  60000/ 200000: 2.4395\n",
            "  70000/ 200000: 1.9687\n",
            "  80000/ 200000: 2.3636\n",
            "  90000/ 200000: 2.1345\n",
            " 100000/ 200000: 1.9419\n",
            " 110000/ 200000: 2.3656\n",
            " 120000/ 200000: 2.0369\n",
            " 130000/ 200000: 2.4437\n",
            " 140000/ 200000: 2.3271\n",
            " 150000/ 200000: 2.2033\n",
            " 160000/ 200000: 1.9513\n",
            " 170000/ 200000: 1.8305\n",
            " 180000/ 200000: 2.1005\n",
            " 190000/ 200000: 1.8946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#debug\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)),g,p)"
      ],
      "metadata": {
        "id": "DCrp5LTHTAHU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm parameters for evaluation\n",
        "with torch.no_grad():\n",
        "  emb=C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0],-1)\n",
        "  hpreact = embcat @W1 + b1\n",
        "  bnmean = hpreact.mean(0,keepdim=True)\n",
        "  bnvar = hpreact.var(0,keepdim=True)"
      ],
      "metadata": {
        "id": "QqwtfQhfQnzD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def split_loss(split):\n",
        "  x,y = { \n",
        "      'train': (Xtr, Ytr),\n",
        "      'val': (Xdev,Ydev),\n",
        "      'test': (Xte,Yte),\n",
        "  }[split]\n",
        "  emb = C[x] #(N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0],-1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain* (hpreact - bnmean) * (bnvar+1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "  \n",
        "split_loss('train')\n",
        "split_loss('test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Ny7UJrPjQ6",
        "outputId": "bb1db4d7-9109-4a1f-b710-9778152cc8e3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0714404582977295\n",
            "test 2.1091206073760986\n"
          ]
        }
      ]
    }
  ]
}