{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashegde/notebooks/blob/main/LinearLM_and_tiny_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yC091qfC5iT"
      },
      "source": [
        "This is a scratch notebook for implementing a linear language model -- for example, as discussed in:\n",
        "\n",
        "\n",
        "*Malach, E. (2023). Auto-regressive next-token predictors are universal learners. arXiv preprint arXiv:2309.06979.*\n",
        "\n",
        "\n",
        "Note that the principal aim here is just to show that simple auto-regressive models can have seemingly non-trivial performance. Such performance can be used as a reference for which to compare and interpret more sophisticated models.\n",
        "\n",
        "At some level, this should not be a surprise. Linear(ized) systems and linear(ized) state space models have formed the foundation of many disciplines -- e.g., control, optimization, time series, econometrics, etc. -- and their usage is widespread in practice.\n",
        "\n",
        "What we show here is that a simple linear model containing just a few thousand parameters can rapidly extract general structure and style from the toy ``Tiny Shakespeare'' dataset. Naturally, this model fails to learn any real language capabaility. Transformer models -- which contain many more parameters -- of course perform much better, as we observed in a previous notebook, but it is still remarkable that structure emerges from such a simple formulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS3T9u6MiQxp"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZYCx9z5Pioj"
      },
      "outputs": [],
      "source": [
        "#Loading the TinyShakespeare Dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osWkIVz2P3IK"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  dataset = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNIqZzXAP-Pc"
      },
      "outputs": [],
      "source": [
        "print(f'The dataset contains {len(dataset)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8IQA1snTfVd"
      },
      "outputs": [],
      "source": [
        "print(type(dataset)) #this dataset is just a big string (not a list of strings, we haven't used split or anything)\n",
        "print(dataset[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7LvCMHkTvCb"
      },
      "outputs": [],
      "source": [
        "# find unique characters to use as our tokens\n",
        "vocab = sorted(list(set(dataset)))\n",
        "vocab_size = len(vocab)\n",
        "print(''.join(vocab))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiwneHFcUZBm"
      },
      "outputs": [],
      "source": [
        "# CHARACTER-LEVEL TOKENIZER\n",
        "\n",
        "class Tokenizer:\n",
        "  \"\"\"\n",
        "  Tokenizer based on an input character list\n",
        "\n",
        "  This class provides functionality for converting text\n",
        "  to character-level tokens.\n",
        "  \"\"\"\n",
        "  def __init__(self, unique_characters: str):\n",
        "    self.vocab = unique_characters\n",
        "    self.character_to_index = {ch:i for i,ch in enumerate(self.vocab)}\n",
        "    self.index_to_character = {i:ch for i,ch in enumerate(self.vocab)}\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Encode the input text into token indices\n",
        "\n",
        "    Args:\n",
        "      text (str): string to tokenize\n",
        "\n",
        "    Returns:\n",
        "      list: list of corresponding token indices\n",
        "    \"\"\"\n",
        "    return [self.character_to_index[c] for c in text]\n",
        "\n",
        "  def decode(self, indices: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Decode the list of token indices into a string\n",
        "\n",
        "    Args:\n",
        "      indices (list[int]): list of integer token indices\n",
        "\n",
        "    Returns:\n",
        "      str: corresponding string\n",
        "    \"\"\"\n",
        "    return \"\".join([self.index_to_character[i] for i in indices])\n",
        "\n",
        "  def get_vocab_size(self):\n",
        "    \"\"\"Returns the size of the vocabulary\"\"\"\n",
        "    return len(self.vocab)\n",
        "\n",
        "tokenizer = Tokenizer(vocab)\n",
        "print(tokenizer.encode('hello there'))\n",
        "print(tokenizer.decode(tokenizer.encode('hello there')))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(\n",
        "    dataset: str,\n",
        "    tokenizer: Tokenizer,\n",
        "    train_frac: float = 0.9,\n",
        ") -> tuple[torch.tensor, torch.tensor]:\n",
        "  \"\"\"\n",
        "  Prepares dataset for model training\n",
        "\n",
        "  Converts the original dataset in the form of a string\n",
        "  into two sequences of token indices -- one for training\n",
        "  and one for testing.\n",
        "\n",
        "  Args:\n",
        "    dataset (str): dataset stored as a singled string\n",
        "    tokenizer (Tokenizer): tokenizer to encode the dataset\n",
        "    train_frac (float): fraction of dataset for training\n",
        "\n",
        "  Returns:\n",
        "    tuple(\n",
        "      train_data (torch.tensor): 1d tensor of token indices for training\n",
        "      val_data (torch.tensor): 1d tensor of token indices for validation\n",
        "    ):\n",
        "  \"\"\"\n",
        "  data = torch.tensor(\n",
        "      tokenizer.encode(dataset),\n",
        "      dtype=torch.long,\n",
        "  )\n",
        "\n",
        "  num_train = int(train_frac * len(data))\n",
        "  return (data[:num_train], data[num_train:])\n"
      ],
      "metadata": {
        "id": "zmhlWwPucca_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umHgmoC1aP8K"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = prepare_data(dataset, tokenizer, 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "Oc4KymUv528s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "  batch_size: int = 64\n",
        "  context_size: int = 8\n",
        "  eps: float = 1e-10\n",
        "  n_embd: int = 32\n",
        "  random_seed = 1337\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "Oa8o2efEAZ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilkwHvvmi8GX"
      },
      "outputs": [],
      "source": [
        "def get_batch(\n",
        "    data: torch.tensor,\n",
        "    config: Config\n",
        ") -> tuple[torch.tensor, torch.tensor]:\n",
        "  \"\"\"\n",
        "  Extracts a minibatch from the input dataset\n",
        "\n",
        "  Args:\n",
        "    data (torch.tensor): 1d tensor of token indices\n",
        "    config (Config): model settings config file\n",
        "\n",
        "  Returns:\n",
        "    tuple(\n",
        "      context (torch.tensor):\n",
        "      targets (torch.tensor):\n",
        "    )\n",
        "  \"\"\"\n",
        "  #select n random starting indices for a sequence of size block_size, where n = batch_size\n",
        "  context_size = config.context_size\n",
        "  batch_size = config.batch_size\n",
        "  device = config.device\n",
        "\n",
        "  ix = torch.randint(len(data)-context_size, (batch_size,))\n",
        "\n",
        "  context = torch.stack([data[i:i+context_size] for i in ix])  # (B,T)\n",
        "  targets = torch.stack([data[i+1:i+context_size+1] for i in ix])  # (B,T)\n",
        "  return context.to(device), targets.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trial: sample a minibatch\n",
        "xb, yb = get_batch(train_data, config)\n",
        "\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "# print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "# print(yb)\n",
        "\n",
        "# print('----')\n",
        "\n",
        "# #below we unpack all of the examples stored in each block in the batch\n",
        "# for b in range(config.batch_size): #b = batch\n",
        "#   for t in range(config.context_size): #t = time\n",
        "#     context = xb[b,:t+1]\n",
        "#     target = yb[b,t]\n",
        "#     print(f'When context is {context.tolist()}, the target is: {target}')"
      ],
      "metadata": {
        "id": "zdEkeg2-mLsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qek_5t9BCwRK"
      },
      "outputs": [],
      "source": [
        "class UpperTriLinear(nn.Linear):\n",
        "  '''\n",
        "  Upper Triangular Linear Layer\n",
        "\n",
        "  Implements an upper triangular linear layer of the form:\n",
        "\n",
        "  x W + b\n",
        "\n",
        "  where W is upper triangular. Note, the implementation below\n",
        "  is based on nn.Linear(in_features, out_features), which has\n",
        "  the following implementation:\n",
        "\n",
        "  x A.T + b\n",
        "\n",
        "  where:\n",
        "\n",
        "  nn.Linear.weight = A, and is (out_features, in_features)\n",
        "  nn.Linear.bias = b, and is (out_features,)\n",
        "\n",
        "  Hence, to apply an upper triangular mask on the matrix multiply,\n",
        "  we must apply a lower triangular mask to A.\n",
        "\n",
        "  '''\n",
        "  def __init__(self, in_features, out_features):\n",
        "      super().__init__(in_features, out_features)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        self.weight.copy_(torch.tril(self.weight))\n",
        "      self.weight.register_hook(lambda grad: grad * torch.tril(torch.ones_like(grad)))\n",
        "\n",
        "class CausalLinearBlock(nn.Module):\n",
        "  '''\n",
        "  Causal Linear Block\n",
        "\n",
        "  Causal linear block that linearly mixes token embeddings *across time*\n",
        "  in a causal manner. The dimensions/channels of the modified tokens\n",
        "  are then subsequently linearly mixed.\n",
        "  '''\n",
        "  def __init__(self, config: Config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.triu_linear = UpperTriLinear(\n",
        "        config.context_size,\n",
        "        config.context_size,\n",
        "    )\n",
        "    self.channel_mixer = nn.Linear(\n",
        "        config.n_embd,\n",
        "        config.n_embd,\n",
        "    )\n",
        "    self.n_embd = config.n_embd\n",
        "    self.context_size = config.context_size\n",
        "\n",
        "  def forward(self, x: torch.tensor) -> torch.tensor:\n",
        "    # x is (B, T, C),\n",
        "    # [B]atches\n",
        "    # [T]okens <= context_size\n",
        "    # [C]hannels := n_embd\n",
        "\n",
        "    B, T, C = x.size()\n",
        "\n",
        "    if T < self.context_size:\n",
        "        x = F.pad(\n",
        "            x,\n",
        "            (0, 0, 0, self.context_size - T),\n",
        "            'constant',\n",
        "            0,\n",
        "        )  # (B, context_size, C)\n",
        "\n",
        "    x = x.transpose(1,2)  # (B, C, context_size)\n",
        "    y = self.triu_linear(x)[:,:,:T].transpose(1,2) # (B, T, C)\n",
        "    return self.channel_mixer(y)\n",
        "\n",
        "\n",
        "class LinearLM(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: Config,\n",
        "        tokenizer: Tokenizer,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.input_embedding = nn.Embedding(\n",
        "            tokenizer.get_vocab_size(),\n",
        "            config.n_embd,\n",
        "        )\n",
        "        self.causal_linear_block = CausalLinearBlock(config)\n",
        "        self.lm_head = nn.Linear(\n",
        "            config.n_embd,\n",
        "            tokenizer.get_vocab_size(),\n",
        "            bias=False,\n",
        "        )    # language model head, final classifier\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.input_embedding.weight = self.lm_head.weight\n",
        "\n",
        "        # initialize parameters\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "    # def _init_weights(self, module):\n",
        "    #     return None\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is (B,T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.context_size, f'input sequence length ({T}) exceeds model context size ({self.config.context_size})'\n",
        "        x = self.input_embedding(idx) # (B, T, n_embd) token embedding for each sequence element\n",
        "        x = self.causal_linear_block(x) #(B, T, n_embd)\n",
        "\n",
        "        # next_token_logits\n",
        "        nt_logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # teacher-forcing supervision\n",
        "        # targets is (B,T)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                nt_logits.view(-1, nt_logits.size(-1)),\n",
        "                targets.view(-1)\n",
        "            )  # (B*T)\n",
        "\n",
        "        return nt_logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T)\n",
        "      for _ in range(max_new_tokens):\n",
        "        # ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens)\n",
        "        idx_cond = idx[:,-self.config.context_size:]\n",
        "        logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "        logits = logits[:,-1,:] # (B,C)\n",
        "        probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "      return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ## Helper functions\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(\n",
        "    model,\n",
        "    data,\n",
        "    config,\n",
        "):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  losses = torch.zeros(eval_iters)\n",
        "  for k in range(eval_iters):\n",
        "    X, Y = get_batch(data, config)\n",
        "    logits, loss = model(X,Y)\n",
        "    losses[k] = loss.item()\n",
        "\n",
        "  model.train()\n",
        "  return losses.mean()"
      ],
      "metadata": {
        "id": "Wp_CqjQQEWCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(config.random_seed)"
      ],
      "metadata": {
        "id": "jKFkoP870FRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgqq3JYINWMB"
      },
      "outputs": [],
      "source": [
        "model = LinearLM(config, tokenizer)\n",
        "model.to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'This model has {sum(p.numel() for p in model.parameters())} parameters')"
      ],
      "metadata": {
        "id": "HC4msBBExvTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimization settings\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "uzZQbOb_HniY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XinBmNtqPaWp"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1000)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch(train_data, config)\n",
        "\n",
        "  #loss\n",
        "  logits, loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    train_loss = estimate_loss(\n",
        "        model,\n",
        "        train_data,\n",
        "        config,\n",
        "    )\n",
        "    val_loss = estimate_loss(\n",
        "        model,\n",
        "        val_data,\n",
        "        config,\n",
        "    )\n",
        "    print(f'iter {step} | train: {train_loss} | test: {val_loss}')\n",
        "\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "idx = idx.to(config.device)\n",
        "print( tokenizer.decode( model.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "id": "6SLA5awvzNKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCaIxSZ7tz9JAjKeeQZaOB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
