{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this notebook, we recreate the simple linear Ornstein-Uhlenbeck example from,\n",
    "\n",
    "Hua, M., Laurière, M., & Vanden-Eijnden, E. (2024). A Simulation-Free Deep Learning \n",
    "Approach to Stochastic Optimal Control. arXiv preprint arXiv:2410.05163.\n",
    "\n",
    "In this reference, the \"simulation-free\" aspect originates from,\n",
    "(1) taking derivatives of the original loss, which is expressed in an importance sampling / Girsanov theory setting,\n",
    "(2) a clever choice of the reference measure (i.e., on-policy), essentially using an independent stop-grad copy of the policy being learned  \n",
    "\n",
    "This example is similar to the linear Ornstein-Uhlenbeck example found in,\n",
    "\n",
    "Nüsken, N., & Richter, L. (2021). Solving high-dimensional Hamilton–Jacobi–Bellman PDEs using neural networks: \n",
    "perspectives from the theory of controlled diffusions and measures on path space. Partial differential equations and applications, 2(4), 48.\n",
    "\n",
    "\n",
    "Work in progress... \n",
    "(2/12/25): with some minor corrections and modifications, we are able to get some fairly strong performance, \n",
    "           at least at the level of loss values (e.g., compare to the rightmost plot in Figure 1 of the primary reference.)\n",
    "           the loss variance can still be quite high.\n",
    "(2/11/25): debugging and trying to reduce the training instability/variance. \n",
    "           the controlled case shows a notable reduction in cost compared to uncontrolled, which is a good sign.\n",
    "\n",
    "These computations were run on a MacBook Pro with an M4 Pro and 24 GB of memory.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.devices()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dynamics\n",
    "d = 20\n",
    "nu = 0.1\n",
    "\n",
    "# state cost\n",
    "# initial distribution - sqrt(1/2) N(0,I) \n",
    "init_stddev = jnp.sqrt(0.5)\n",
    "\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "A_sys = -jnp.eye(d) + nu * jax.random.normal(subkey1, shape=(d,d))\n",
    "B_sys = jnp.eye(d) + nu * jax.random.normal(subkey2, shape=(d,d))\n",
    "\n",
    "def process_step(prev_x: jnp.array, dw: jnp.array, dt: float):\n",
    "    '''\n",
    "    One step of the linear Ornstein-Uhlenbeck process,\n",
    "    discretized via Euler-Maruyama.\n",
    "    \n",
    "    Outputs designed for use with jax.lax.scan.\n",
    "    '''\n",
    "    next_x = prev_x + dt * jnp.matmul(A_sys, prev_x) + jnp.sqrt(dt) * jnp.matmul(B_sys, dw)\n",
    "    return next_x, prev_x\n",
    "\n",
    "process_scan = lambda x, c : process_step(prev_x=x, dw=c[0:d], dt=c[d:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tsteps = 1000\n",
    "dt = 0.01*jnp.ones((num_tsteps,1))\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "dw = jax.random.normal(subkey, shape=(num_tsteps, d))\n",
    "\n",
    "x_init = jnp.zeros((d,))\n",
    "x_final, x_walk = jax.lax.scan(f=process_scan, init=x_init, xs= jnp.concat((dw, dt), axis=1))\n",
    "\n",
    "for i in range(d):\n",
    "    plt.plot(jnp.cumsum(dt), x_walk[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_walk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL\n",
    "\n",
    "def random_layer_params(\n",
    "        key: jax.random.key,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        b_scale: float = 0.0,\n",
    ") -> tuple[jnp.array]:\n",
    "    \n",
    "    w_key, b_key = jax.random.split(key, 2)\n",
    "    stdv = 1/jnp.sqrt(in_dim) \n",
    "    return (\n",
    "        jax.random.uniform(\n",
    "            w_key,\n",
    "            shape=(out_dim, in_dim),\n",
    "            minval=-stdv,\n",
    "            maxval=stdv,\n",
    "        ),\n",
    "        b_scale * jax.random.uniform(\n",
    "            b_key,\n",
    "            shape=(out_dim,),\n",
    "            minval=-stdv,\n",
    "            maxval=stdv,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def init_network_params(key: jax.random.key, layer_sizes: list[int], b_scale: int = 0.0) -> list[int]:\n",
    "    keys = jax.random.split(key, len(layer_sizes))\n",
    "    return [\n",
    "        random_layer_params(k, in_dim, out_dim, b_scale) \n",
    "        for in_dim, out_dim, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)\n",
    "    ]\n",
    "\n",
    "def predict_single(params: jnp.array, x: jnp.array, t: float) -> jnp.array:\n",
    "    def relu(z: jnp.array):\n",
    "        return jnp.maximum(0, z)\n",
    "    def leaky_relu(z: jnp.array, alpha: float = 0.1):\n",
    "        return jnp.maximum(alpha*z, z)\n",
    "    def tanh(z: jnp.array):\n",
    "        return jnp.tanh(z)\n",
    "    def sigmoid(z: jnp.array):\n",
    "        return 1/(1 + jnp.exp(-z))\n",
    "    def silu(z: jnp.array):\n",
    "        return z * sigmoid(z)\n",
    "    \n",
    "    h = jnp.append(x, t)\n",
    "    readin_w, readin_b = params[0]\n",
    "    u = jnp.dot(readin_w, h) + readin_b\n",
    "    h = leaky_relu(u)\n",
    "\n",
    "    for (w,b) in params[1:-1]:\n",
    "        u = jnp.dot(w, h) + b\n",
    "        #u = (u - u.mean()) / u.std()\n",
    "        h = relu(u) \n",
    "        # h = tanh(u)\n",
    "    \n",
    "    readout_w, readout_b = params[-1]\n",
    "    return jnp.dot(readout_w, h) + readout_b\n",
    "\n",
    "batch_predict = jax.vmap(predict_single, in_axes=(None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dt_xinits_and_dw(\n",
    "        key: jax.random.key,\n",
    "        n_timesteps: float,\n",
    "        final_time: float,\n",
    "        n_walkers: int,\n",
    ") -> tuple:\n",
    "    key, subkey1, subkey2, subkey3 = jax.random.split(key, 4)\n",
    "\n",
    "    # time grid\n",
    "    time_grid = jnp.sort(jax.random.uniform(subkey1, shape=(n_timesteps-2,), minval=0, maxval=final_time))\n",
    "    time_grid = jnp.insert(time_grid, jnp.array((0, n_timesteps-2)), jnp.array((0, final_time)))\n",
    "    dt = time_grid[1:] - time_grid[:-1]\n",
    "\n",
    "    # x_inits\n",
    "    x_inits = init_stddev * jax.random.normal(key=subkey2, shape=(n_walkers, d))\n",
    "\n",
    "    # noise\n",
    "    dw = jax.random.normal(key=subkey3, shape=(n_walkers, n_timesteps-1, d))\n",
    "\n",
    "    return dt, x_inits, dw\n",
    "\n",
    "def controlled_step(policy_params: list[jnp.array], prev_x: jnp.array, dw: jnp.array, dt: float, t: float):\n",
    "    # next_x @ time=t+dt, prev_x @ time=t\n",
    "    next_x = prev_x + dt * jnp.matmul(A_sys, prev_x) + dt * jnp.matmul(B_sys, predict_single(policy_params, prev_x, t)) + jnp.sqrt(dt)*jnp.matmul(B_sys, dw)\n",
    "    return next_x, prev_x\n",
    "\n",
    "def simulate_walker(params: list[jnp.array], x_init: jnp.array, dt: jnp.array, dw: jnp.array):\n",
    "\n",
    "    controlled_scan = lambda x, c : controlled_step(policy_params=params, prev_x=x, dw=c[0:d], dt=c[d], t=c[d+1])\n",
    "    \n",
    "    # time\n",
    "    t = jnp.insert(jnp.cumsum(dt[:-1]), 0, 0) # t[1:] = t[:-1]+dt[:-1]\n",
    "    # forward simulation with policy (but no gradients needed)\n",
    "    x_final, x_path = map(\n",
    "            jax.lax.stop_gradient,\n",
    "            jax.lax.scan(f=controlled_scan, init=x_init, xs=jnp.concat((dw, dt[:, None], t[:, None]), axis=1))\n",
    "    )\n",
    "\n",
    "    # policy evaluations on on states\n",
    "    policy_eval = batch_predict(params, x_path, t) \n",
    "\n",
    "    A = 0.5 * (dt * (policy_eval**2).sum(axis=-1)).sum() \n",
    "    Abar = jax.lax.stop_gradient(A)\n",
    "    C = (jnp.sqrt(dt) * (policy_eval * dw).sum(axis=-1)).sum()\n",
    "    Bbar = 0.0 # 0.5 * (dt * (x_path**2).sum(axis=-1)).sum()  # state cost \n",
    "    g = x_final.sum() # final state cost\n",
    "\n",
    "    return A + (Abar + Bbar + g) * C\n",
    "\n",
    "batch_walkers = jax.vmap(simulate_walker, in_axes=(None, 0, None, 0))\n",
    "\n",
    "def simulation_free_loss(\n",
    "    params: list[jnp.array],\n",
    "    x_inits: jnp.array,\n",
    "    dt: jnp.array,\n",
    "    dw: jnp.array,\n",
    "):\n",
    "    # simulate a batch of walkers\n",
    "    return batch_walkers(params, x_inits, dt, dw).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIMIZATION\n",
    "\n",
    "def moving_average_step(\n",
    "    past_state: list[jnp.array],\n",
    "    new_state: list[jnp.array],\n",
    "    beta: float = 0.9,\n",
    ") -> list[jnp.array]:\n",
    "    return [\n",
    "        (beta * w_ps + (1-beta) * w_ns, beta * b_ps +(1-beta) * b_ns)\n",
    "        for (w_ps, b_ps), (w_ns, b_ns) in zip(past_state, new_state)\n",
    "    ]\n",
    "    \n",
    "def clip_grads(grads: list[jnp.array], clip_threshold: float = 1.0) -> list[jnp.array]:\n",
    "     return [(_clip(w, clip_threshold), _clip(b, clip_threshold)) for (w, b) in grads]\n",
    "    \n",
    "def _clip(g: jnp.array, clip_threshold: float, eps: float = 1e-7) -> jnp.array:\n",
    "    g_norm = jnp.max(jnp.array([jnp.linalg.norm(g), eps]))\n",
    "    return jnp.min(jnp.array([clip_threshold/g_norm, 1.0])) * g\n",
    "\n",
    "def gradient_step(\n",
    "    params: list[jnp.array],\n",
    "    grads: list[jnp.array],\n",
    "    lr: float,\n",
    "    weight_decay: float = 0,\n",
    ") -> list[jnp.array]:\n",
    "    \n",
    "    return [\n",
    "        ( (1 - lr * weight_decay) * w - lr * dw, (1 - lr * weight_decay) * b - lr * db) for (w, b), (dw, db) in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "def adamw_step(\n",
    "    params: list[jnp.array],\n",
    "    momentum: list[jnp.array],\n",
    "    velocity: list[jnp.array],\n",
    "    lr: float,\n",
    "    weight_decay: float = 0,\n",
    "    eps: float = 1e-8,\n",
    ") -> list[jnp.array]:\n",
    "    \n",
    "    adjusted_grads = [(w_g / (jnp.sqrt(w_v) + eps), b_g / (jnp.sqrt(b_v) + eps)) for (w_g, b_g), (w_v, b_v) in zip(momentum, velocity)]\n",
    "    return [\n",
    "        ( (1 - weight_decay) * w - lr * dw, (1 - weight_decay) * b - lr * db) for (w, b), (dw, db) in zip(params, adjusted_grads)\n",
    "    ] \n",
    "\n",
    "def cosine_lr_scheduler(min_lr: float, max_lr: float, current_epoch: int, epochs_per_cycle: int, decay_rate: float = 1.0) -> float:\n",
    "     adjusted_max_lr = decay_rate**current_epoch * max_lr\n",
    "     return min_lr + 0.5 * (adjusted_max_lr - min_lr) * (1 + jnp.cos( (current_epoch % epochs_per_cycle) / epochs_per_cycle * jnp.pi))\n",
    "\n",
    "def decay_lr_scheduler(lr: float, decay_rate: float = 0.95) -> float:\n",
    "     return lr * decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training settings\n",
    "\n",
    "# network\n",
    "layers = [d+1, 128, 128, d]\n",
    "b_scale = 1.0\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "params = init_network_params(subkey, layers, b_scale)\n",
    "\n",
    "# loss\n",
    "\n",
    "n_timesteps = 500\n",
    "final_time = 2\n",
    "n_walkers = 2048\n",
    "\n",
    "# optimization\n",
    "n_epochs = 1000\n",
    "# momentum = None\n",
    "# velocity = None\n",
    "momentum = [(jnp.zeros_like(w), jnp.zeros_like(b)) for w,b in params]\n",
    "velocity = [(jnp.zeros_like(w), jnp.zeros_like(b)) for w,b in params]\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "min_lr = 1e-4\n",
    "max_lr = 3e-4\n",
    "lr = max_lr\n",
    "epochs_per_cycle = 100\n",
    "weight_decay = 1e-5\n",
    "\n",
    "lr_scheduler = partial(\n",
    "    cosine_lr_scheduler,\n",
    "    min_lr=min_lr,\n",
    "    max_lr=max_lr,\n",
    "    epochs_per_cycle=epochs_per_cycle,\n",
    "    decay_rate = 1.0,\n",
    ")\n",
    "\n",
    "\n",
    "value_and_grad_fn = jax.value_and_grad(simulation_free_loss, argnums=0)\n",
    "\n",
    "@jax.jit\n",
    "def update(\n",
    "    params: list[jnp.array],\n",
    "    dt: jnp.array,\n",
    "    x_inits: jnp.array,\n",
    "    dw: jnp.array,\n",
    "    lr: float,\n",
    "    momentum: list[jnp.array],\n",
    "    velocity: list[jnp.array],\n",
    "    epoch: int,\n",
    ")->tuple:\n",
    "    \n",
    "    loss_val, grads = value_and_grad_fn(params, x_inits, dt, dw)\n",
    "    clipped_grads = clip_grads(grads, 1.0)\n",
    "\n",
    "    # adamW, based on:\n",
    "    # Loshchilov, I., & Hutter, F. (2017). Fixing weight decay regularization in adam.\n",
    "    # arXiv preprint arXiv:1711.05101, 5.\n",
    "\n",
    "    momentum = moving_average_step(momentum, clipped_grads, beta1)\n",
    "    velocity = moving_average_step(velocity, [(w**2, b**2) for w, b in clipped_grads], beta2)\n",
    "    \n",
    "    # bias corrections for moving average initializations\n",
    "    mhat = [(w / (1-beta1**(epoch+1)),  b / (1-beta1**(epoch+1))) for w, b in momentum]\n",
    "    vhat = [(w / (1-beta2**(epoch+1)),  b / (1-beta2**(epoch+1))) for w, b in velocity]\n",
    "\n",
    "    return loss_val, adamw_step(params, mhat, vhat, lr, weight_decay, eps=1e-4), momentum, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    dt, x_inits, dw = generate_dt_xinits_and_dw(\n",
    "        key=subkey,\n",
    "        n_timesteps=n_timesteps,\n",
    "        final_time=final_time,\n",
    "        n_walkers=n_walkers,\n",
    "    )\n",
    "\n",
    "    loss_val, params, momentum, velocity = update(\n",
    "        params=params,\n",
    "        dt=dt,\n",
    "        x_inits=x_inits,\n",
    "        dw=dw,\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        velocity=velocity,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "\n",
    "    t_epoch = time.time() - t_start\n",
    "    print(f'epoch {epoch} | lr {lr:0.3e} | loss {loss_val:0.5e} | epoch time: {t_epoch:0.3e}s')\n",
    "    lr = lr_scheduler(current_epoch=epoch)\n",
    "    losses.append(loss_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controlled_step(policy_params: list[jnp.array], prev_x: jnp.array, dw: jnp.array, dt: float, t: float):\n",
    "    next_x = prev_x + dt * jnp.matmul(A_sys, prev_x) + dt * jnp.matmul(B_sys, predict_single(policy_params, prev_x, t)) + jnp.sqrt(dt)*jnp.matmul(B_sys, dw)\n",
    "    return next_x, prev_x\n",
    "\n",
    "controlled_scan = lambda x, c : controlled_step(policy_params=params, prev_x=x, dw=c[0:d], dt=c[d], t=c[d+1])  \n",
    "\n",
    "def uncontrolled_step(prev_x: jnp.array, dw: jnp.array, dt: float):\n",
    "    next_x = prev_x + dt * jnp.matmul(A_sys, prev_x) + jnp.sqrt(dt)*jnp.matmul(B_sys, dw)\n",
    "    return next_x, prev_x\n",
    "\n",
    "uncontrolled_scan = lambda x, c : uncontrolled_step(prev_x=x, dw=c[0:d], dt=c[d])   \n",
    "# forward simulation with policy (but no gradients needed)\n",
    "num_tsteps = 200\n",
    "dt = 0.01*jnp.ones((num_tsteps,1))\n",
    "t = jnp.insert(jnp.cumsum(dt[:-1]), 0, 0)\n",
    "\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "dw = jax.random.normal(subkey1, shape=(num_tsteps, d))\n",
    "x_init = init_stddev * jax.random.normal(subkey2, shape=(d,))\n",
    "\n",
    "xc_final, xc_path = jax.lax.scan(f=controlled_scan, init=x_init, xs=jnp.concat((dw, dt, t[:, None]), axis=1))\n",
    "xuc_final, xuc_path = jax.lax.scan(f=uncontrolled_scan, init=x_init, xs=jnp.concat((dw, dt), axis=1))\n",
    "\n",
    "for i in range(d):\n",
    "    plt.plot(t, xc_path[:,i])\n",
    "\n",
    "for i in range(d):\n",
    "    plt.plot(t, xuc_path[:,i], c='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# compare final costs:\n",
    "print('Cost objective being minimized')\n",
    "print(f'Uncontrolled: {xuc_final.sum()}')\n",
    "print(f'Controlled: {xc_final.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
