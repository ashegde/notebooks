{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we work through section 3 of __Neubig, G. (2017). Neural machine translation and sequence-to-sequence models: A tutorial. arXiv preprint arXiv:1703.01619__. \n",
        "\n",
        "A nice reference that lays out the foundations of SMT is __Della Pietra, V. J. (1994). The mathematics of statistical machine translation: Parameter estimation. Using Large Corpora, 223__ . "
      ],
      "metadata": {
        "id": "LGcgXi2G54xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Machine Translation\n",
        "\n",
        "Neural machine translation (NMT) is a subset of the broader framework of statistical machine translation (SMT). Consider two languages $\\mathcal{E}$ (e.g., English) and $\\mathcal{F}$ (e.g., French) to be sets finite strings with elements in some discrete vocabularies $\\mathcal{V}_{\\mathcal{E}}$ and $\\mathcal{V}_{\\mathcal{F}}$. We consider the outcome or sample space to be $\\Omega = \\mathcal{E} \\otimes \\mathcal{F}$. Thus $(e,f) \\in \\Omega$ is a pair of English and French sentences. \n",
        "\n",
        "Suppose our objective is to translate a source sentence $f = (f_1,...,f_m)$ where $f_i \\in \\mathcal{V}_{\\mathcal{F}}$ into an english sentence $e = (e_1,...,e_n)$ where $e_j \\in \\mathcal{V}_{\\mathcal{E}}$. We assume that in the natural world, language data are assumed paired random variables $(E,F)$, generated according to some unknown probability measure $(E,F) \\sim \\textrm{Pr}_{E \\times F}$ that assigns higher probability to translationally equivalent pairs. We assume that the training data $\\{(E^{i},F^{i})\\}_{i=1}^N$ are independent and identically distributed samples from this unknown probability measure $\\textrm{Pr}_{E \\times F}$.\n",
        "\n",
        "More specifically, we are interested in estimating the conditional $E|F$, whose distribution can be formulated via Bayes' theorem as:\n",
        "\n",
        "$\\textrm{Pr}(E=e|F=f) = \\frac{ \\textrm{Pr}_{E}(E=e) \\textrm{Pr}_{F|E}(F=f|E=e)}{\\textrm{Pr}_{\\mathcal{F}}(F=f)}$\n",
        "\n",
        "Hence for a given realization $F=f$, we may estimate an optimal $e$ via the Maximum A Posteriori estimate:\n",
        "\n",
        "$\\hat{e} = \\textrm{argmax}_e \\textrm{Pr}(E=e|F=f)$, \n",
        "\n",
        "which is equivalent to,\n",
        "\n",
        "$\\hat{e} = \\textrm{argmax}_e  \\textrm{Pr}_{E}(E=e) \\textrm{Pr}_{F|E}(F=f|E=e)$.\n",
        "\n",
        "The modeling objective is to build tractable probability models to approximate $E$ and $F|E$. In NMT, these ``tractable models'' correspond to neural networks.\n",
        "\n",
        "Note that this framework extends far beyond just languages. For example, it seems plausible that any blackbox system that takes as input a signal and outputs another signal could be treated in a similar matter. For some reason, this reminds me of input-output control, e.g., as in Vidyasagar's book.\n"
      ],
      "metadata": {
        "id": "RT2mmGet6Kg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram language models\n",
        "\n",
        "Here we consider $N$-gram Markov models for an individual language $\\mathcal{E}$. And our objective is to build a model of $\\textrm{Pr}(E=e)$. This is often done by factorizing the sentence $e=(e_1,...,e_T)$ made up of $T$ words in the following manner:\n",
        "\n",
        " $\\textrm{Pr}(E_1=e_1, ..., E_T = e_T) = \\textrm{Pr}(E_1=e_1) \\cdot \\textrm{Pr}(E_2 = e_2 | E_1=e_1) \\cdot ... \\cdot \\textrm{Pr}(E_T = e_T | E_{T-1} = e_{t-1},...,E_1=e_1)$,\n",
        "\n",
        "where we have dropped the subscripts of the probability measure for convenience. \n",
        "\n",
        "A practical challenge with the above approach is that items being conditioned on continually increase in size. Hence, large sequences with size $T >> 0$ can become unmanageable. To address this, we may make additional assumptions about the above probability model -- specifically, with regard to conditional independence structure. First, as we will show below, we augment the vocabulary with a special token to act as a placeholder/start/stopping indicator. Second, we enforce a context window, whereby the target element is statistically independent of all other elements given the elements within its context window,"
      ],
      "metadata": {
        "id": "rS0gn5PGCRC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Dict, Any, Tuple"
      ],
      "metadata": {
        "id": "U82pU8TWDQb1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we construct a data module that downloads, loads, and stores the relevant training data. Note that in the example below, we look at the English language. And although we have describe the problem above as an English-French translation, in the example below we look at English-German."
      ],
      "metadata": {
        "id": "kqDak5LiJrn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataModule:\n",
        "  '''\n",
        "  Helper module that downloads, loads, and partitions a paired German-English dataset.\n",
        "  '''\n",
        "  url = \"http://phontron.com/data/\"\n",
        "  filename = \"iwslt-en-de-preprocessed.tar.gz\"\n",
        "\n",
        "  def __init__(self, dir):\n",
        "      self.dir = dir \n",
        "      self.path = self.dir + '/' +self.filename\n",
        "\n",
        "  def download_data(self):\n",
        "    # create directories and download dataset\n",
        "    if not os.path.exists(self.dir):\n",
        "      os.mkdir(self.dir)\n",
        "    if not os.path.exists(self.path):\n",
        "      content = requests.get(self.url + self.filename).content\n",
        "      with open(self.path, \"wb\") as f:\n",
        "        f.write(content)\n",
        "    with tarfile.open(self.path) as f:\n",
        "      f.extractall(path=self.dir)\n",
        "  \n",
        "  def format(self, line: str) -> List[str]:\n",
        "    #return line.strip().translate(str.maketrans('', '', string.punctuation)).lower().split() #strip out punctuation and case-sensitivity\n",
        "    return re.findall(r\"[\\w']+|[.,!?;]\", line) #punctuation gets its own entry, case-sensitivity remains\n",
        "    \n",
        "\n",
        "  def setup(self):\n",
        "    # load data\n",
        "    ds_train = {}\n",
        "    ds_valid = {}\n",
        "    ds_test = {}\n",
        "    with open(self.dir+'/en-de/train.en-de.en', 'r') as f1:\n",
        "      ds_train['en'] = [self.format(line) for line in f1]\n",
        "    with open(self.dir+'/en-de/train.en-de.de', 'r') as f2:\n",
        "      ds_train['de'] = [self.format(line) for line in f2]\n",
        "    with open(self.dir+'/en-de/valid.en-de.en', 'r') as f1:\n",
        "      ds_valid['en'] = [self.format(line) for line in f1]\n",
        "    with open(self.dir+'/en-de/valid.en-de.de', 'r') as f2:\n",
        "      ds_valid['de'] = [self.format(line) for line in f2]\n",
        "    with open(self.dir+'/en-de/test.en-de.en', 'r') as f1:\n",
        "      ds_test['en'] = [self.format(line) for line in f1]\n",
        "    with open(self.dir+'/en-de/test.en-de.de', 'r') as f2:\n",
        "      ds_test['de'] = [self.format(line) for line in f2]\n",
        "    \n",
        "    self.train = ds_train\n",
        "    self.valid = ds_valid\n",
        "    self.test = ds_test"
      ],
      "metadata": {
        "id": "BqVJY1BpEMcs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/data\"\n",
        "dm = DataModule(dir)\n",
        "dm.download_data()\n",
        "dm.setup()"
      ],
      "metadata": {
        "id": "ebEqhjP8PUbc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#observing what the data looks like\n",
        "print(dm.train['en'][:10])"
      ],
      "metadata": {
        "id": "xmc2kM4cyQiE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c29e62-c287-4415-a692-4c587e30d7bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['With', 'vibrant', 'video', 'clips', 'captured', 'by', 'submarines', ',', 'David', 'Gallo', 'takes', 'us', 'to', 'some', 'of', \"Earth's\", 'darkest', ',', 'most', 'violent', ',', 'toxic', 'and', 'beautiful', 'habitats', ',', 'the', 'valleys', 'and', 'volcanic', 'ridges', 'of', 'the', \"oceans'\", 'depths', ',', 'where', 'life', 'is', 'bizarre', ',', 'resilient', 'and', 'shockingly', 'abundant', '.'], ['David', 'Gallo', 'This', 'is', 'Bill', 'Lange', '.', \"I'm\", 'Dave', 'Gallo', '.'], ['And', \"we're\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.'], [\"We've\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'Titanic', \"that's\", 'ever', 'been', 'seen', ',', 'and', \"we're\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.'], ['The', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'Titanic', 'even', 'though', \"it's\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', \"it's\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.'], ['And', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.'], ['When', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.'], ['Most', 'of', 'the', 'planet', 'is', 'ocean', 'water', '.'], ['The', 'average', 'depth', 'is', 'about', 'two', 'miles', '.'], ['Part', 'of', 'the', 'problem', ',', 'I', 'think', ',', 'is', 'we', 'stand', 'at', 'the', 'beach', ',', 'or', 'we', 'see', 'images', 'like', 'this', 'of', 'the', 'ocean', ',', 'and', 'you', 'look', 'out', 'at', 'this', 'great', 'big', 'blue', 'expanse', ',', 'and', \"it's\", 'shimmering', 'and', \"it's\", 'moving', 'and', \"there's\", 'waves', 'and', \"there's\", 'surf', 'and', \"there's\", 'tides', ',', 'but', 'you', 'have', 'no', 'idea', 'for', 'what', 'lies', 'in', 'there', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will construct an $n$-gram model. Recall, $n$-gram models limit their context to the previous $(n-1)$ words. Words further in the past are assumed conditionally independent of the current word given the context."
      ],
      "metadata": {
        "id": "ZI8D5DcHg8Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As written in the tutorial, several standard for smoothing perform interpolation recursively.\n",
        "# Hence, when we deal with n-grams, we really want to keep track of all m-grams from m=1,...,n. \n",
        "\n",
        "\n",
        "# \n",
        "n=2 \n",
        "a = 0.05\n",
        "# build vocabulary\n",
        "vocab = ['<S>', '<E>']+sorted(list(set([word for sublist in dm.train['en'] for word in sublist]))) \n",
        "stoi = {s:i for i,s in enumerate(vocab)} #mapping strings to indices\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# build counts\n",
        "counts = {}# dictionary containing counts of all m-gram levels from m=1,...,n.\n",
        "for m in range(n):\n",
        "  for sentence in dm.train['en']:\n",
        "    s = ['<S>'] + sentence + ['<E>'] # start and end symbols\n",
        "    for jj in zip(*[s[ii:] for ii in range(m+1)]): # sliding a context window across the sentence, the window size is \"m\"\n",
        "      mgram = tuple(jj)\n",
        "      counts[mgram] = counts.get(mgram, 0)+1\n",
        "\n",
        "# build prob estimates\n",
        "logprobs = {}\n",
        "\n",
        "sum_unigrams = sum([v for k,v in counts.items() if len(k)==1]) \n",
        "for k,v in counts.items():\n",
        "  if len(k) == 1: #singletons\n",
        "    logprobs[k] = np.log(v) - np.log(sum_unigrams)\n",
        "  else:\n",
        "    logprobs[k] = np.log(v) - np.log(counts[k[:-1]])"
      ],
      "metadata": {
        "id": "ldUaGIhPVItZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "p = np.array( [logprobs[tuple([w])] for w in vocab] )\n",
        "print( np.exp(p).sum() )\n",
        "\n",
        "context = ['<S>']\n",
        "p =  np.array( [logprobs.get( tuple(context+[w]), -np.inf) for w in vocab] )\n",
        "print( np.exp(p).sum() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD3XwUpbxMr4",
        "outputId": "023ac579-1e48-4499-c812-ac5d7634135b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0000000000000004\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = ['<S>','The']\n",
        "counts.get(tuple(context), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7I65BhD0nmx",
        "outputId": "98497d2e-b37d-4304-ef83-ba86bd2a148b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4041"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming n>=2, let's just look at the bigram statistics\n",
        "bigram_stats = [b[1] for b in counts.items() if len(b[0])==2]\n",
        "(min(bigram_stats),\n",
        " max(bigram_stats),\n",
        " sum(bigram_stats)/len(bigram_stats) )"
      ],
      "metadata": {
        "id": "46Atm5wPz-Ci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd95026-38b4-4468-ae84-7ff3a5caf26d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 88929, 4.34032408395664)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(sorted(bigram_stats, reverse=True)[0:1000], bins = 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "B_p4rypmhmZD",
        "outputId": "20fbd921-ae90-4ab8-abb8-9693e596e0d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.ones((vocab_size,)) * 1/vocab_size # 0-th order (0-gram) probabilities for smoothing unknowns\n",
        "\n",
        "context = ['<S>']*(n-1)\n",
        "for m in range(n):\n",
        "  logpm = np.array( [logprobs.get(tuple(context+[w])[-1-m:], -np.inf) for w in vocab] )\n",
        "  p = (1-a) * np.exp( logpm ) + a * p"
      ],
      "metadata": {
        "id": "v1sJl5Acr4F1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G_R5bZ5vX8V",
        "outputId": "3787f1cd-331c-4f1b-94fd-e255d22d5608"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000000000002"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the $n$-gram counts, we now construct our probabilities. We will use interpolation among probabilities to handle smoothing. This latter aspect is critical. As observed above, language statistics can be extremely sparse -- most of the $n$-grams occur infrequently in the data. Getting good estimates of these probabilities in what are essentially the tail of the distribution is challenging.  \n"
      ],
      "metadata": {
        "id": "DfBlYm-40lj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packaging the above into a class\n",
        "\n",
        "class NGram:\n",
        "  '''\n",
        "  n-gram class that computes and stores a dictionary of m-gram counts, for m=1,...,n.\n",
        "  We store all levels of m-grams so that we can perform smoothing, i.e., \n",
        "  interpolating probabilities at test time on sentences that did not occur in the training set. \n",
        "  '''\n",
        "\n",
        "  def __init__(self, n: int, x_train: List, a: float):\n",
        "    self.n = n # n-gram order\n",
        "    self.x_train = x_train \n",
        "    self.a = a #interpolation constant\n",
        "\n",
        "  def fit(self):\n",
        "    # build vocabulary\n",
        "    vocab = ['<S>', '<E>', '<Unk>']+sorted(list(set([word for sublist in self.x_train for word in sublist]))) \n",
        "\n",
        "    self.stoi = {s:i for i,s in enumerate(vocab)} #mapping strings to indices\n",
        "    self.itos = {i:s for s,i in self.stoi.items()}\n",
        "    self.vocab = vocab\n",
        "    self.vocab_size = len(vocab)\n",
        "\n",
        "    # build counts\n",
        "    counts = {('<Unk>',): 1}# dictionary containing counts of all m-gram levels from m=1,...,n.\n",
        "    for m in range(self.n):\n",
        "      for sentence in self.x_train:\n",
        "        s = ['<S>'] + sentence + ['<E>'] # start and end symbols\n",
        "        for jj in zip(*[s[ii:] for ii in range(m+1)]): # sliding a context window across the sentence, the window size is \"m\"\n",
        "          mgram = tuple(jj)\n",
        "          counts[mgram] = counts.get(mgram, 0)+1\n",
        "    \n",
        "    # build prob estimates\n",
        "    logprobs = {}\n",
        "\n",
        "    sum_unigrams = sum([v for k,v in counts.items() if len(k)==1]) \n",
        "    for k,v in counts.items():\n",
        "      if len(k) == 1: #singletons\n",
        "        logprobs[k] = np.log(v) - np.log(sum_unigrams)\n",
        "      else:\n",
        "        logprobs[k] = np.log(v) - np.log(counts[k[:-1]])\n",
        "    \n",
        "    self.logprobs = logprobs\n",
        "\n",
        "  def step(self, context: List[str]):\n",
        "    # outputs probability distribution over the vocab given the context\n",
        "    # probabilities recursively interpolated for smoothing\n",
        "    #assert(len(context) == self.n-1, f'context length should correspond to the previous {self.n-1} words')\n",
        "    p = np.ones((self.vocab_size,)) * 1/vocab_size # 0-th order (0-gram) probabilities for smoothing\n",
        "    \n",
        "    for m in range(self.n):\n",
        "      logpm = np.array( [self.logprobs.get(tuple(context+[w])[-1-m:], -np.inf) for w in self.vocab] )\n",
        "      p = (1-self.a) * np.exp( logpm ) + self.a * p\n",
        "    return p\n",
        "\n",
        "  def forward(self, seq: List[str]):\n",
        "    #computes the negative log likelihood of a sequence\n",
        "    context = ['<S>']*(self.n-1)\n",
        "    #nseq = len(seq)\n",
        "    nll = 0\n",
        "    for w in seq:\n",
        "      pnext = self.step(context)\n",
        "      nll -= np.log( pnext[self.stoi.get(w, 2)] ) #If cannot find word w, return the unknown\n",
        "      context = context[1:]+[w]\n",
        "\n",
        "    out = nll \n",
        "    return out\n",
        "\n",
        "  def sample(self, seq=None):\n",
        "    # samples the next value given the context\n",
        "\n",
        "    if seq == None:\n",
        "      out = ['<S>']\n",
        "      context = ['<S>']*(self.n-1)\n",
        "    else:\n",
        "      out = seq\n",
        "      context = [seq[-self.n+m] if  m-self.n >= len(seq) else '<S>' for m in range(self.n) ]\n",
        "    while out[-1] != '<E>':\n",
        "      pnext = self.step(context)\n",
        "      ix = np.random.multinomial(1, pnext).argmax()\n",
        "      w = self.itos[int(ix)]\n",
        "      out.append(w)\n",
        "      context = context[1:]+[w]\n",
        "\n",
        "    return out\n",
        "\n",
        "  def __call__(self, seq: List[str]):\n",
        "    return self.forward(seq)\n"
      ],
      "metadata": {
        "id": "5qBABg0FtDPu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NGram(2, dm.train['en'], 0.05)\n",
        "model.fit()"
      ],
      "metadata": {
        "id": "Hpi-ENtL6N8S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling sequences from the model\n",
        "nsamp = 10\n",
        "for ii in range(nsamp):\n",
        "  s = model.sample()\n",
        "  print(' '.join(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hyj0oVUpqoM",
        "outputId": "dc6f02ee-b737-4e2b-b7f2-7de9db43bdfa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<S> The looking for helping them when Sputnik . It's called Hacking Team has just like it's not negotiate effectively . I'm like Josh and lost when losing an overshoot , not just crossed in my own performances from him in the CEO of course , and obsessive , I would you , and logistical problem for this is strips after the cell in not only one planet today . <E>\n",
            "<S> They move . <E>\n",
            "<S> They've never being installed my behavior actually found a tour with other things that has done and we cannot buy still , which speaks . <E>\n",
            "<S> So <S> It becomes the third . <E>\n",
            "<S> So what it's true environmental future hold diapers and create foundations for your brain that Americans every year . <E>\n",
            "<S> And this version of democracy is . <E>\n",
            "<S> This terrified and they've been separated from the simple parts per capita . <E>\n",
            "<S> And , bringing their families , if you put in how they may collect extra large figures , this body language we're able to know , we might see here today that very few blocks present . <E>\n",
            "<S> You and technology . <E>\n",
            "<S> Forget her musical notes on planets in Thailand from these leaders should devote all of the naked strangers to blame Murray won his light areas where we have to the one are completely new change , it is it . Okay , a healthy , who's in the fact , parenting always watching a pregnancy , which is my body is that challenge was was wrong . <E>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the negative log likelihood on the training data and on the test data.\n",
        "# here we used a reduced dataset to speed up the evaluation\n",
        "\n",
        "Xtr = dm.train['en'][:100]\n",
        "Xte = dm.test['en'][:100]\n",
        "\n",
        "num_train_words = sum([len(s) for s in Xtr])\n",
        "num_test_words = sum([len(s) for s in Xte])\n",
        "\n",
        "nll_train = sum([model(s) for s in Xtr]) / num_train_words\n",
        "nll_test = sum([model(s) for s in Xte]) / num_test_words\n",
        "\n",
        "print(f'train nll: {nll_train}, test nll: {nll_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbPSBLEaDQS_",
        "outputId": "031c9afe-1b74-4167-9e37-72f6656bbee1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train nll: 4.289743415678976, test nll: 6.144923651011172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For references, a purely random model predicts each word in a sentence independently with probability $1/\\textrm{vocab_size}$, which leads to a per-word negative loglikelihood of:"
      ],
      "metadata": {
        "id": "qYKNufPXnHRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nll_rand = - np.log(1/model.vocab_size)\n",
        "print(nll_rand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3tt4DjqnW4o",
        "outputId": "1a9e18e5-0da6-4851-ca44-f254e148e5fe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.69629905620902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems likely that the test negative log likelihood is exaggerated by the presence of words that do not exist in the training set. We have addressed this above by using an \"$<\\textrm{Unk}>$\" symbol to provide support for words not encountered during training. The probability of this special character has been assigned to a very small value. Thus, the negative log likelihood for sentences with unknown words should be very high. As described in Neubig's tutorial, there are many ways of improving on this simple characterization. In the interest of time, however, we will leave it here for now. "
      ],
      "metadata": {
        "id": "5i7Ad__eoDEF"
      }
    }
  ]
}