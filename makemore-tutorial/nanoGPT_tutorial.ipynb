{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yC091qfC5iT"
      },
      "source": [
        "The material in this notebook follows Andrej Karpathy's nanoGPT tutorial (https://www.youtube.com/watch?v=kCc8FmEb1nY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS3T9u6MiQxp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZYCx9z5Pioj"
      },
      "outputs": [],
      "source": [
        "#Loading the TinyShakespeare Dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osWkIVz2P3IK"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNIqZzXAP-Pc"
      },
      "outputs": [],
      "source": [
        "print(f'The dataset contains {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8IQA1snTfVd"
      },
      "outputs": [],
      "source": [
        "print(type(text)) #this dataset is just a big string (not a list of strings, we haven't used split or anything)\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7LvCMHkTvCb"
      },
      "outputs": [],
      "source": [
        "# extract the unique characters/symbols/atoms that build the dataset\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiwneHFcUZBm"
      },
      "outputs": [],
      "source": [
        "# TOKENIZER\n",
        "# map characters to integers and vice versa\n",
        "# this corresponds to a character level tokenizer\n",
        "# state of the art methods tend to use subword level tokenizers\n",
        "#\n",
        "# note that although simple, character level tokenizers tend to produce very long sequences compared to other tokenizers.\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # converts string (list of characters) to integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) #converts list of integers to a single string (list of characters)\n",
        "\n",
        "print(encode('hello there'))\n",
        "print(decode(encode('hello there')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-SYmufNYJMN"
      },
      "outputs": [],
      "source": [
        "# encoding the dataset and storing as a tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "plt.hist(data, vocab_size)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DINvv5fZZRTV"
      },
      "outputs": [],
      "source": [
        "print(f'The most occuring character is: \"{itos[1]}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFMe62mdZoA8"
      },
      "outputs": [],
      "source": [
        "## Training and validation split of the data\n",
        "\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJIZwk2ZbHqk"
      },
      "source": [
        "In most applications, the training dataset is massive to be fed into the transformer all at once. Instead, training is performed by randomly sampling blocks or subsequences from the training set of a specified size, which is refered to here as a $\\texttt{block_size}$ or a $\\texttt{context_length}$. These blocks are ordered and contain multiple pieces of information. For example, consider  $\\texttt{block_size} = N$, e.g., \n",
        "\n",
        "$[1,...,N]$\n",
        "\n",
        "As we are interested in predicting next tokens, we can create the following list of training examples from that single block with the format $\\texttt{target}$ | $\\texttt{input}$:\n",
        "\n",
        "$1|$\n",
        "\n",
        "$2|1$\n",
        "\n",
        "$3|2,1$\n",
        "\n",
        "...\n",
        "\n",
        "$N | N-1,....,1$\n",
        "\n",
        "Hence the training using contexts of size $1$ all the way up to $\\texttt{block_size}$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umHgmoC1aP8K"
      },
      "outputs": [],
      "source": [
        "# here is an example of the above\n",
        "\n",
        "block_size = 8 # context length\n",
        "print(f'The first training block is {train_data[:block_size+1]}')\n",
        "print(f'From this block, we can construct the following prediction cases.')\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f'When the input is {context}, the target is {target}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I7_iojTig0_"
      },
      "source": [
        "Recall the analogy with joint probabilities. For a sequence $\\{t_1,...,t_N\\}$, the joint probability has the following factorization:\n",
        "\n",
        "$p(t_1,...,t_N) = p(t_1) p(t_2|t_1) p(t_3| t_2,t_1) ... p(t_{N} | t_{N-1},...,t_1)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0llrlM9qj4LK"
      },
      "source": [
        "Transformers are not trained on individual blocks, but rather minibatches of multiple blocks. Hence, the $\\texttt{batch_size}$ is also an important hyperparameter. The primary reason for doing this is for efficiency.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w3Npa5ECkdr"
      },
      "outputs": [],
      "source": [
        "#Some default hyperparameters, which we will change throughout\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "A2AO_B4IxZFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilkwHvvmi8GX"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent blocks/sequences will we process in parallel?\n",
        "block_size = 8 # what is the max context length for predictions?\n",
        "\n",
        "def get_batch(opt):\n",
        "  data = train_data if opt == 'train' else val_data\n",
        "  #select n random starting indices for a sequence of size block_size, where n = batch_size\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]) #create each block at each starting location in ix\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #create targets for each block in the batch\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x, y \n",
        "\n",
        "#next, we sample a batch from the training data set\n",
        "xb, yb = get_batch('train') \n",
        "\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "#below we unpack all of the examples stored in each block in the batch\n",
        "for b in range(batch_size): #b = batch\n",
        "  for t in range(block_size): #t = time\n",
        "    context = xb[b,:t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f'When input is {context.tolist()}, the target is: {target}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_sOeG1yCwK_"
      },
      "outputs": [],
      "source": [
        " ## Helper functions\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qek_5t9BCwRK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM96tpnNyTCB"
      },
      "source": [
        "In order to build up to a Transformer network, we will first recall the Bigram language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63qpX3B-yasf"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table.\n",
        "    # This lookup table is basically a transition probability matrix, where the probabilities are replaced by logits.\n",
        "    # Hence, entry (i,j) of the table corresponds to the logit associated with transitioning from the current token i the next token j.\n",
        "    # Thus the i-th row contains the logits (probability distribution over the vocabulary) for the next token.\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #(C,C)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    logits = self.token_embedding_table(idx) #(B,T,C)\n",
        "    # logits(b,t,:) contains the logits for predicting the next token given that we are currently at token t in batch b.\n",
        "    # these logits correspond to a probability distribution over the vocabulary.\n",
        "    \n",
        "    # we need to reshape things to work with F.cross_entropy\n",
        "    \n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "    # max_new_tokens is how many new tokens to sample\n",
        "\n",
        "    ## Note: In this simple bigram model, we only need to keep track of the latest indices and not the entire history.\n",
        "    ##       In the following code, we actually keep track of all of the indices in 'idx' and continually append to it.\n",
        "    ##       This is because we will need this capability in subsequent models that have more context.\n",
        "    for _ in range(max_new_tokens):\n",
        "      # make the predictions (probability distributions over next entries)\n",
        "      logits, loss = self(idx) # logits is (B,T,C), loss is (B*T)\n",
        "      # focus on the last time step because that is the only context needed for bigram predictions\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      # convert logits to probs\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      # sample\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      # appending new sampled indices to our current set\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb,yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-vHMOtAYW8S"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s06pI6HHcdWl"
      },
      "outputs": [],
      "source": [
        "preds = m.generate(xb, 20)\n",
        "\n",
        "decode(preds[0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2TOvxb4e-49"
      },
      "outputs": [],
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5_wsQr4gB6C"
      },
      "source": [
        "Obviously, this purely random model spits out gibberish. Now we will train this model on the tinyShakespeare dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfLLzcKNz_4F"
      },
      "outputs": [],
      "source": [
        "# define some hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9ErtX4pgJCL"
      },
      "outputs": [],
      "source": [
        "# PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1dYiJUvmwNs"
      },
      "outputs": [],
      "source": [
        "for steps in range(10000):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWS-PFWPoNTm"
      },
      "outputs": [],
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZDh5UBdoUN6"
      },
      "source": [
        "While the model is still incomprehensible, it is considerably improved over the untrained model. At least some of the structure of has been learned. Bigram models are quite poor, but they at least provide a benchmark for comparison. We need longer contexts than just 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4DEN456qCCS"
      },
      "source": [
        "## Efficient implementations of self-attention use the following trick\n",
        "\n",
        "Basically, to ensure causal interactions via masking. By causal, we mean that the future cannot communicate with the past. This is a form of autoregression and highlights the role of lower triangular matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_18S2DU6otof"
      },
      "outputs": [],
      "source": [
        "# toy example\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 #batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aOayWc_rO5U"
      },
      "outputs": [],
      "source": [
        "# We want a causal temporal average:\n",
        "#  x[b,t] = mean_{i<=t} x[b,i]\n",
        "# bow = bag of words, averaging over previous words\n",
        "# all notion of order is loss, so some information is loss\n",
        "xbow = torch.zeros((B,T,C)) \n",
        "for b in range(B): #for each sequence in the batch\n",
        "  for t in range(T): #for each time\n",
        "    xprev = x[b,:t+1] #(t,C), includes the t-th entry\n",
        "    xbow[b,t] = torch.mean(xprev,0) #(C) average across the previous times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KSu2yFttQIp"
      },
      "outputs": [],
      "source": [
        "# A more efficient formulation than for loops\n",
        "# causal averaging operator\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / a.sum(axis=1, keepdim=True)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUG9pTpysbqN"
      },
      "outputs": [],
      "source": [
        "wei = torch.tril(torch.ones(T,T)) #temporal averaging weights\n",
        "wei = wei / wei.sum(axis=1, keepdim=True)\n",
        "\n",
        "xbow2 = wei @ x #(T,T) @ (B,T,C) ----> (B,T,C) due to broadcasting, batch calculations done in parallel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9eRDNlHuDKC"
      },
      "outputs": [],
      "source": [
        "print( torch.allclose(xbow,xbow2) )\n",
        "print(xbow[0], xbow2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxKB3rrDwKxM"
      },
      "outputs": [],
      "source": [
        "# We can write the same averaging operator with a softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n",
        "\n",
        "# The averaging operator 'wei' is currently a uniform average.\n",
        "# In reality, we may want a more general weighted average (but still causal), where the weights can be learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbnBxf7EzyPh"
      },
      "source": [
        "## Expanding the Bigram Model\n",
        "We will begin to expand the previous bigram model with seemingly redundant information. These \"redundant\" aspects, such as token and position embeddings become important when dealing with Transformer models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTTsxR1v0iae"
      },
      "outputs": [],
      "source": [
        "# define some hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMI1fnAbz0yZ"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table.\n",
        "    # This lookup table is basically a transition probability matrix, where the probabilities are replaced by logits.\n",
        "    # Hence, entry (i,j) of the table corresponds to the logit associated with transitioning from the current token i the next token j.\n",
        "    # Thus the i-th row contains the logits (probability distribution over the vocabulary) for the next token.\n",
        "\n",
        "    # first we embed each input token into euclidean space (rather than using a one-hot vector)\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #(vocab_size,n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) #(block_size, n_embd)\n",
        "    # Note the token_embedding only embeds information about the value of the token, but not its position in the context\n",
        "    # Hence, we also include a position_embedding in the same space, basically as an additional learnable degree of freedom to account for the positioning within the context.\n",
        "    # Importantly, the position_embedding is only used for position information and not for conveying token value. Hence it is the same within the batch.\n",
        "\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    # logits(b,t,:) contains the logits for predicting the next token given that we are currently at token t in batch b.\n",
        "    # these logits correspond to a probability distribution over the vocabulary.\n",
        "    \n",
        "    # we need to reshape things to work with F.cross_entropy\n",
        "    \n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "    # max_new_tokens is how many new tokens to sample\n",
        "\n",
        "    ## Note: In this simple bigram model, we only need to keep track of the latest indices and not the entire history.\n",
        "    ##       In the following code, we actually keep track of all of the indices in 'idx' and continually append to it.\n",
        "    ##       This is because we will need this capability in subsequent models that have more context.\n",
        "    for _ in range(max_new_tokens):\n",
        "      # make the predictions (probability distributions over next entries)\n",
        "      logits, loss = self(idx) # logits is (B,T,C), loss is (B*T)\n",
        "      # focus on the last time step because that is the only context needed for bigram predictions\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      # convert logits to probs\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      # sample\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      # appending new sampled indices to our current set\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "logits, loss = m(xb,yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlO3Wiga4783"
      },
      "source": [
        "## The crux of attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsj1uHr94-Co"
      },
      "outputs": [],
      "source": [
        "#recap\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 #batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "# x[i,:,:] is a (T,C) snapshot matrix, where the t-th row x[i,t,:] contains the C-dimensional embedding of the t-th token\n",
        "# so the rows of x[i,:,:] index the time steps\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x # causal time averages of each dimension of the embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wum6S0Ig8gqI"
      },
      "source": [
        "With attention, we want to achieve something similar but more flexible. We still want to have information propogate in a causal way -- meaning that future tokens cannot influence the past. In constrast with the previous time averaging, however, we want this to be achieved in a data-dependent way.\n",
        "\n",
        "To do this, we abstract the notion of weights and averaging to the notion of affinities and similarity. Such concepts have been fruitfull applied with respect to manifold learning and kernel methods on graphs. The transformer perspective is as follows.\n",
        "\n",
        "Each token emits a query vector and a key vector in the same space. Key vectors contain information pertaining to the token's identity and query vectors contain information about what the token is looking for. We use \"information\" here in the loose sense as these key and query vectors constructed by a linear transformation of the input, which is learned from data. Each token communicates with all other tokens by taking the dot product between its own query vector and all other key vectors. These dot products are \"affinities\".\n",
        "\n",
        "The key and query vectors are created by linear transformation of the input vector. Why not use an emmbedding table? Perhapsto reduce the number of parameters.\n",
        "\n",
        "Attention is a communication mechanism between tokens in a sequence. We can interpret this as a directed graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdNQvT46_ggt"
      },
      "outputs": [],
      "source": [
        "# A single attention head\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 #batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False) # (C, head_size)\n",
        "query = nn.Linear(C, head_size, bias=False) # (C, head_size)\n",
        "value = nn.Linear(C, head_size, bias=False) #(C, head_size)\n",
        "## x[i,:,:] is a (T,C) matrix of time snapshots in the rows\n",
        "## x[i,:,:] @ key is a (T,head_size)\n",
        "k = key(x) #(B,T,head_size)\n",
        "q = query(x) #(B,T,head_size)\n",
        "v = value(x) #(B,T,head_size)\n",
        "wei = q @ k.swapaxes(-2,-1) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
        "wei = wei * (head_size)**(-0.5) #scaling / normalization\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril==0, float('-inf')) #ensures causal masking\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ v #(B,T, head_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7PG5JaLNPAb"
      },
      "outputs": [],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq6TvY3PQufo"
      },
      "outputs": [],
      "source": [
        "## Formalizing\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" One-headed self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # torch will recognize that 'tril' is not a model parameter\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape #batch, time, channel\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.query(x) #(B,T,C)\n",
        "    v = self.value(x) #(B,T,C)\n",
        "\n",
        "    wei = q @ k.swapaxes(-2,-1) * C**(-0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "\n",
        "    wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
        "    out = wei @ v #(B,T,C)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbYJKLxFTJtX"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = SelfAttentionModel()\n",
        "logits, loss = m(xb,yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF8rfS9P_d0A"
      },
      "outputs": [],
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lL5QP3QAUvC"
      },
      "outputs": [],
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_izZFAtAwgt"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple self-attention heads in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([h(x) for h in self.heads], dim=-1) #results of each head concatenated together along the last axis (channel dimension)\n",
        "\n",
        "mtmp = MultiHeadAttention(4, int(n_embd/4) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG_zFvApBjzX"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, int(n_embd/4) ) #n_embd needs to be divisible by 4\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = MultiHeadSelfAttentionModel()\n",
        "logits, loss = m(xb,yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq-H0AAZFb8P"
      },
      "outputs": [],
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbNUT89yIvJo"
      },
      "outputs": [],
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKxaswEwJHZt"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  \"\"\" just a linear layer and subsequent nonlinearity\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd,n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXXOIHccJx_U"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiHeadAttention(4, int(n_embd/4) ) #n_embd needs to be divisible by 4\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffwd(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = MultiHeadSelfAttentionModel()\n",
        "logits, loss = m(xb,yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdOuSmoAKBjO"
      },
      "outputs": [],
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 10000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 1000 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhnQ4U0NTXak"
      },
      "source": [
        "## Full Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfj_Zn6FSrBJ"
      },
      "outputs": [],
      "source": [
        "# define some hyperparameters\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 128\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 180 #needs to be divisible by n_head\n",
        "\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHybGbU3Ldhk"
      },
      "outputs": [],
      "source": [
        "## Formalizing\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" One-headed self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # torch will recognize that 'tril' is not a model parameter\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape #batch, time, channel\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.query(x) #(B,T,C)\n",
        "    v = self.value(x) #(B,T,C)\n",
        "\n",
        "    wei = q @ k.swapaxes(-2,-1) * C**(-0.5) # (B, T, C) @ (B, C, T) = (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "\n",
        "    wei = F.softmax(wei, dim=-1) #(B,T,T)\n",
        "    weu = self.dropout(wei)\n",
        "    out = wei @ v #(B,T,C)\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" multiple self-attention heads in parallel \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1) #results of each head concatenated together along the last axis (channel dimension)\n",
        "    out = self.dropout( self.proj(out) )\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\" just a linear layer and subsequent nonlinearity\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential( \n",
        "        nn.Linear(n_embd, 4 * n_embd),    #the choice of 4 here is empirical\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\" Transformer block: communication follwed by computation \"\"\"\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) #communication (matcing keys and queries among tokens to get values for each token)\n",
        "    self.ffwd = FeedForward(n_embd) #computation (operating on the token values)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = x+self.sa( self.ln1(x) ) #residual connections\n",
        "    x = x+self.ffwd( self.ln2(x) )\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgqq3JYINWMB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # Let B = batch_size, T = time, C = channels = vocab_size \n",
        "    # idx and targets are both integer tensors of dimension (B,T) \n",
        "    B,T = idx.shape\n",
        "    tok_embd = self.token_embedding_table(idx) #(B,T,n_embd)\n",
        "    pos_embd = self.position_embedding_table(torch.arange(T, device=device)) #(T, n_embd)\n",
        "    x = tok_embd+pos_embd #(B,T,n_embd) + (T,n_embd) = (B,T,n_embd)\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) --> corresponds to logistic model on the embedded tokens\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T) \n",
        "      loss = F.cross_entropy(logits, targets) #(B,T)      \n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is of dim (B,T) whose (b,t)th entry corresponds to the vocabulary index in batch b at time t\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      #ensure we stay within scope (context never exceeds block_size, i.e., the context = the most recent upt-to-block_size tokens) \n",
        "      idx_cond = idx[:,-block_size:] \n",
        "      logits, loss = self(idx_cond) # logits is (B,T,C), loss is (B*T)\n",
        "      logits = logits[:,-1,:] # (B,C)\n",
        "      probs = F.softmax(logits, dim=-1) #(B,C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = MultiHeadSelfAttentionModel()\n",
        "m.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'This model has {sum(p.numel() for p in m.parameters())} parameters')"
      ],
      "metadata": {
        "id": "HC4msBBExvTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XinBmNtqPaWp"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "for step in range(max_iters):\n",
        "  #minibatch\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  #loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    outs = estimate_loss(m)\n",
        "    print(f\"iter {step} | train: {outs['train']} | test: {outs['val']}\")\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "idx = idx.to(device)\n",
        "print( decode( m.generate(idx, max_new_tokens=500)[0].tolist() ))"
      ],
      "metadata": {
        "id": "6SLA5awvzNKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}